{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":"<p>This site contains the documentation for the <code>behavysis_pipeline</code> program.</p>"},{"location":"examples/batch_pipeline.html","title":"Analysing a Folder of Experiments","text":"<p>All outcomes for experiment processing is stored in csv files in the <code>proj_dir/diagnostics</code> folder. These files store the outcome and process description (i.e. error explanations) of all experiments.</p>"},{"location":"examples/batch_pipeline.html#loading-in-all-relevant-packages","title":"Loading in all relevant packages","text":"<pre><code>from behavysis_pipeline import Project\nfrom behavysis_pipeline.processes import *\n</code></pre> <p>TODO</p>"},{"location":"examples/single_pipeline.html","title":"Single Image Analysis Pipeline","text":""},{"location":"examples/single_pipeline.html#loading-in-all-relevant-packages","title":"Loading in all relevant packages","text":"<pre><code>from cellcounter.pipelines.pipeline_funcs import (\n    cell_mapping_pipeline,\n    cellc1_pipeline,\n    cellc2_pipeline,\n    cellc3_pipeline,\n    cellc4_pipeline,\n    cellc5_pipeline,\n    cellc6_pipeline,\n    cellc7_pipeline,\n    cellc8_pipeline,\n    cellc9_pipeline,\n    cellc10_pipeline,\n    cellc11_pipeline,\n    cellc_coords_only_pipeline,\n    cells2csv_pipeline,\n    coords2points_raw_pipeline,\n    coords2points_trfm_pipeline,\n    group_cells_pipeline,\n    img_fine_pipeline,\n    img_overlap_pipeline,\n    img_rough_pipeline,\n    img_trim_pipeline,\n    make_mask_pipeline,\n    ref_prepare_pipeline,\n    registration_pipeline,\n    tiff2zarr_pipeline,\n    transform_coords_pipeline,\n)\nfrom cellcounter.utils.proj_org_utils import (\n    get_proj_fp_model,\n    update_configs,\n)\n</code></pre>"},{"location":"examples/single_pipeline.html#set-the-input-filenamefolder-and-project-folder-to-use","title":"Set the input filename/folder and project folder to use","text":"<pre><code>in_fp = \"/path/to/input_file.tiff\"\nproj_dir = \"/path/to/proj_dir\"\n</code></pre>"},{"location":"examples/single_pipeline.html#load-project-object","title":"Load project object","text":"<pre><code>pfm = get_proj_fp_model(proj_dir)\n</code></pre>"},{"location":"examples/single_pipeline.html#update-configs","title":"Update configs","text":"<p>Also see Configs for more parameters to modify and their descriptions.</p> <pre><code>update_configs(\n    pfm,\n    # REFERENCE\n    # RAW\n    # REGISTRATION\n    ref_orient_ls=(-2, 3, 1),\n    ref_z_trim=(None, None, None),\n    ref_y_trim=(None, None, None),\n    ref_x_trim=(None, None, None),\n    z_rough=3,\n    y_rough=6,\n    x_rough=6,\n    z_fine=1,\n    y_fine=0.6,\n    x_fine=0.6,\n    z_trim=(None, None, None),\n    y_trim=(None, None, None),\n    x_trim=(None, None, None),\n    # MASK\n    # OVERLAP\n    # CELL COUNTING\n    tophat_sigma=10,\n    dog_sigma1=1,\n    dog_sigma2=4,\n    gauss_sigma=101,\n    thresh_p=60,\n    min_threshd=100,\n    max_threshd=9000,\n    maxima_sigma=10,\n    min_wshed=1,\n    max_wshed=700,\n)\n</code></pre>"},{"location":"examples/single_pipeline.html#run-pipeline","title":"Run Pipeline","text":"<p>For more information, see Pipeline</p>"},{"location":"examples/single_pipeline.html#single-line-version","title":"Single line version","text":"<pre><code>all_pipeline(pfm, in_fp, overwrite=overwrite)\n</code></pre>"},{"location":"examples/single_pipeline.html#manually-run-each-function-version","title":"Manually run each function version","text":"<pre><code># Making zarr from tiff file(s)\ntiff2zarr_pipeline(pfm, in_fp, overwrite=overwrite)\n# Preparing reference images\nref_prepare_pipeline(pfm, overwrite=overwrite)\n# Preparing image itself\nimg_rough_pipeline(pfm, overwrite=overwrite)\nimg_fine_pipeline(pfm, overwrite=overwrite)\nimg_trim_pipeline(pfm, overwrite=overwrite)\n# Running Elastix registration\nregistration_pipeline(pfm, overwrite=overwrite)\n# Running mask pipeline\nmake_mask_pipeline(pfm, overwrite=overwrite)\n# Making overlap chunks in preparation for cell counting\nimg_overlap_pipeline(pfm, overwrite=overwrite)\n# Counting cells\ncellc1_pipeline(pfm, overwrite=overwrite)\ncellc2_pipeline(pfm, overwrite=overwrite)\ncellc3_pipeline(pfm, overwrite=overwrite)\ncellc4_pipeline(pfm, overwrite=overwrite)\ncellc5_pipeline(pfm, overwrite=overwrite)\ncellc6_pipeline(pfm, overwrite=overwrite)\ncellc7_pipeline(pfm, overwrite=overwrite)\ncellc8_pipeline(pfm, overwrite=overwrite)\ncellc9_pipeline(pfm, overwrite=overwrite)\ncellc10_pipeline(pfm, overwrite=overwrite)\ncellc11_pipeline(pfm, overwrite=overwrite)\ncellc_coords_only_pipeline(pfm, overwrite=overwrite)\n# Converting maxima from raw space to refernce atlas space\ntransform_coords_pipeline(pfm, overwrite=overwrite)\n# Getting Region ID mappings for each cell\ncell_mapping_pipeline(pfm, overwrite=overwrite)\n# Grouping cells\ngroup_cells_pipeline(pfm, overwrite=overwrite)\n# Exporting cells_agg parquet as csv\ncells2csv_pipeline(pfm, overwrite=overwrite)\n</code></pre>"},{"location":"examples/single_pipeline.html#optional-run-visual-checks","title":"Optional: Run visual checks","text":"<pre><code># Running visual checks\ncoords2points_raw_pipeline(pfm)\ncoords2points_trfm_pipeline(pfm)\n</code></pre>"},{"location":"installation/installing.html","title":"Installing","text":"<p>Step 1:</p> <p>Install conda by visiting the Miniconda downloads page and following the prompts to install on your system.</p> <p>Open the downloaded miniconda file and follow the installation prompts.</p> <p>Step 2:</p> <p>Open a terminal (Mac or Linux) or Anaconda PowerShell Prompt (Windows) and verify that conda has been installed with the following command.</p> <pre><code>conda --version\n</code></pre> <p>A response like <code>conda xx.xx.xx</code> indicates that it has been correctly installed.</p> <p>Step 3:</p> <p>Update conda and use the libmamba solver (makes downloading conda programs MUCH faster):</p> <pre><code>conda update -n base conda\nconda install -n base conda-libmamba-solver\nconda config --set solver libmamba\n</code></pre> <p>Step 4:</p> <p>Install packages that help Jupyter notebooks read conda environments:</p> <pre><code>conda install -n base nb_conda nb_conda_kernels\n</code></pre> <p>Step 5:</p> <p>Install the <code>behavysis_pipeline</code> conda environment (download here).</p> <pre><code>conda env create -f path/to/conda_env.yaml\n</code></pre> <p>Step 6:</p> <p>Install the <code>DEEPLABCUT</code> conda environment (download here).</p> <pre><code>conda env create -f path/to/DEEPLABCUT.yaml\n</code></pre> <p>Step 7:</p> <p>Install the <code>simba</code> conda environment (download here).</p> <pre><code>conda env create -f path/to/simba_env.yaml\n</code></pre>"},{"location":"installation/running.html","title":"Running","text":"<p>Step 1:</p> <p>Open a terminal (Mac or Linux) or Anaconda PowerShell Prompt (Windows)</p> <p>Step 2:</p> <p>Activate the program environment with the following command:</p> <pre><code>conda activate behavysis_pipeline_env\n</code></pre> <p>Step 3:</p> <p>You can now use the <code>behavysis</code> package in a Jupyter kernel or regular Python script.</p> <p>See here for examples of Jupyter notebooks to run behaviour analysis.</p> <p>See here for examples of Jupyter notebooks to train behaviour classifiers.</p> <p>See here for a tutorial of <code>behavysis</code>'s workflow.</p> <p>See here for API documentation.</p> <p>Note</p> <p>To run jupyter, run the following command in the terminal</p> <pre><code>jupyter-lab\n</code></pre> <p>This will open a browser to <code>http://127.0.0.1:8888/lab</code>, where you can run jupyter notebooks.</p> <p>You can also run jupyter notebooks in VS Code.</p>"},{"location":"installation/uninstalling.html","title":"Uninstalling","text":"<p>For more information about how to uninstall conda, see here.</p> <p>Step 1:</p> <p>Open a terminal (Mac or Linux) or Anaconda PowerShell Prompt (Windows)</p> <p>Step 2:</p> <p>To uninstall the <code>behavysis_pipeline_env</code>, <code>DEEPLABCUT</code>, and <code>simba</code> conda envs, run the following commands:</p> <pre><code>conda env remove -n behavysis_pipeline_env\nconda env remove -n DEEPLABCUT\nconda env remove -n simba\n</code></pre> <p>Step 3:</p> <p>To remove conda, enter the following commands in the terminal.</p> <pre><code>conda install anaconda-clean\nanaconda-clean --yes\n\nrm -rf ~/anaconda3\nrm -rf ~/opt/anaconda3\nrm -rf ~/.anaconda_backup\n</code></pre> <p>Step 5: Edit your bash or zsh profile so conda it does not look for conda anymore. Open each of these files (note that not all of them may exist on your computer), <code>~/.zshrc</code>, <code>~/.zprofile</code>, or <code>~/.bash_profile</code>, with the following command.</p> <pre><code>open ~/.zshrc\nopen ~/.zprofile\nopen ~/.bash_profile\n</code></pre>"},{"location":"installation/updating.html","title":"Updating","text":"<p>Step 1: Download the <code>conda_env.yaml</code> file from here</p> <p>Step 2: Run the following command to update <code>behavysis_pipeline</code>:</p> <pre><code>conda env update -f conda_env.yaml --prune\n</code></pre>"},{"location":"reference/config_params_model.html","title":"Config params model","text":""},{"location":"reference/config_params_model.html#cellcounter.utils.config_params_model.ConfigParamsModel","title":"<code>cellcounter.utils.config_params_model.ConfigParamsModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Pydantic model for registration parameters.</p> Source code in <code>cellcounter/utils/config_params_model.py</code> <pre><code>class ConfigParamsModel(BaseModel):\n    \"\"\"\n    Pydantic model for registration parameters.\n    \"\"\"\n\n    # NOTE: can set extra as \"forbid\" to prevent extra keys\n    model_config = ConfigDict(\n        extra=\"ignore\",\n        # arbitrary_types_allowed=True,\n        validate_default=True,\n        use_enum_values=True,\n    )\n\n    # REFERENCE\n    atlas_dir: str = ATLAS_DIR\n    ref_version: str = RefVersions.AVERAGE_TEMPLATE_25.value\n    annot_version: str = AnnotVersions.CCF_2016_25.value\n    map_version: str = MapVersions.ABA_ANNOTATIONS.value\n    # RAW\n    zarr_chunksize: tuple[int, int, int] = PROC_CHUNKS\n    # REGISTRATION\n    ref_orient_ls: tuple[int, int, int] = (1, 2, 3)\n    ref_z_trim: tuple[int | None, int | None, int | None] = (None, None, None)\n    ref_y_trim: tuple[int | None, int | None, int | None] = (None, None, None)\n    ref_x_trim: tuple[int | None, int | None, int | None] = (None, None, None)\n    z_rough: int = 3\n    y_rough: int = 6\n    x_rough: int = 6\n    z_fine: float = 1.0\n    y_fine: float = 0.6\n    x_fine: float = 0.6\n    z_trim: tuple[int | None, int | None, int | None] = (None, None, None)\n    y_trim: tuple[int | None, int | None, int | None] = (None, None, None)\n    x_trim: tuple[int | None, int | None, int | None] = (None, None, None)\n    lower_bound: tuple[int, int] = (100, 0)\n    upper_bound: tuple[int, int] = (5000, 5000)\n    # MASK\n    mask_gaus_blur: int = 1\n    mask_thresh: int = 300\n    # CELL COUNT TUNING CROP\n    tuning_z_trim: tuple[int | None, int | None, int | None] = (0, 100, None)\n    tuning_y_trim: tuple[int | None, int | None, int | None] = (0, 2000, None)\n    tuning_x_trim: tuple[int | None, int | None, int | None] = (0, 2000, None)\n    # OVERLAP\n    overlap_depth: int = DEPTH\n    # CELL COUNTING\n    tophat_sigma: int = 10\n    dog_sigma1: int = 1\n    dog_sigma2: int = 4\n    large_gauss_sigma: int = 101\n    threshd_value: int = 60\n    min_threshd_size: int = 100\n    max_threshd_size: int = 10000\n    maxima_sigma: int = 10\n    min_wshed_size: int = 1\n    max_wshed_size: int = 1000\n    # VISUAL CHECK\n    heatmap_raw_radius: int = 5\n    heatmap_trfm_radius: int = 3\n    # COMBINE ARRAYS\n    combine_cellc_z_trim: tuple[int | None, int | None, int | None] = (0, 10, None)\n    combine_cellc_y_trim: tuple[int | None, int | None, int | None] = (None, None, None)\n    combine_cellc_x_trim: tuple[int | None, int | None, int | None] = (None, None, None)\n\n    @model_validator(mode=\"after\")\n    def _validate_trims(self):\n        # Orient validation\n        vect = np.array(self.ref_orient_ls)\n        vect_abs = np.abs(vect)\n        vect_abs_sorted = np.sort(vect_abs)\n        assert np.all(vect_abs_sorted == np.array([1, 2, 3]))\n        # TODO: Size validation\n        # TODO: Trim validation\n        return self\n\n    @classmethod\n    def read_fp(cls, fp: str):\n        model = cls.model_validate(read_json(fp))\n        return model\n\n    def update(self, **kwargs):\n        return self.model_validate(self.model_copy(update=kwargs))\n\n    @classmethod\n    def update_file(cls, fp: str, **kwargs):\n        \"\"\"\n        Reads the json file in `fp`, updates the parameters with `kwargs`,\n        writes the updated parameters back to `fp` (if there are any updates),\n        and returns the model instance.\n        \"\"\"\n        configs = cls.model_validate(read_json(fp))\n        # Updating and saving if kwargs is not empty\n        if kwargs != {}:\n            configs = cls.model_validate(configs.model_copy(update=kwargs))\n            write_json(fp, configs.model_dump())\n        return configs\n</code></pre>"},{"location":"reference/config_params_model.html#cellcounter.utils.config_params_model.ConfigParamsModel.update_file","title":"<code>update_file(fp, **kwargs)</code>  <code>classmethod</code>","text":"<p>Reads the json file in <code>fp</code>, updates the parameters with <code>kwargs</code>, writes the updated parameters back to <code>fp</code> (if there are any updates), and returns the model instance.</p> Source code in <code>cellcounter/utils/config_params_model.py</code> <pre><code>@classmethod\ndef update_file(cls, fp: str, **kwargs):\n    \"\"\"\n    Reads the json file in `fp`, updates the parameters with `kwargs`,\n    writes the updated parameters back to `fp` (if there are any updates),\n    and returns the model instance.\n    \"\"\"\n    configs = cls.model_validate(read_json(fp))\n    # Updating and saving if kwargs is not empty\n    if kwargs != {}:\n        configs = cls.model_validate(configs.model_copy(update=kwargs))\n        write_json(fp, configs.model_dump())\n    return configs\n</code></pre>"},{"location":"reference/pipeline_funcs.html","title":"Pipeline funcs","text":""},{"location":"reference/pipeline_funcs.html#cellcounter.pipeline.pipeline.Pipeline","title":"<code>cellcounter.pipeline.pipeline.Pipeline</code>","text":"Source code in <code>cellcounter/pipeline/pipeline.py</code> <pre><code>class Pipeline:\n    # Clusters\n    # heavy (few workers - carrying high RAM computations)\n    heavy_n_workers = 2\n    heavy_threads_per_worker = 1\n    # busy (many workers - carrying low RAM computations)\n    busy_n_workers = 6\n    busy_threads_per_worker = 2\n    # gpu\n    _gpu_cluster = LocalCUDACluster\n    # GPU enabled cell funcs\n    cellc_funcs: Type[CpuCellcFuncs] = GpuCellcFuncs\n\n    ###################################################################################################\n    # SETTING PROCESSING CONFIGS (NUMBER OF WORKERS, GPU ENABLED, ETC.)\n    ###################################################################################################\n\n    @classmethod\n    def heavy_cluster(cls):\n        return LocalCluster(n_workers=cls.heavy_n_workers, threads_per_worker=cls.heavy_threads_per_worker)\n\n    @classmethod\n    def busy_cluster(cls):\n        return LocalCluster(n_workers=cls.busy_n_workers, threads_per_worker=cls.busy_threads_per_worker)\n\n    @classmethod\n    def gpu_cluster(cls):\n        return cls._gpu_cluster()\n\n    @classmethod\n    def set_gpu(cls, enabled: bool = True):\n        if enabled:\n            cls._gpu_cluster = LocalCUDACluster\n            cls.cellc_funcs = GpuCellcFuncs\n        else:\n            cls._gpu_cluster = lambda: LocalCluster(\n                n_workers=cls.heavy_n_workers, threads_per_worker=cls.heavy_threads_per_worker\n            )\n            cls.cellc_funcs = CpuCellcFuncs\n\n    ###################################################################################################\n    # UPDATE CONFIGS\n    ###################################################################################################\n\n    @classmethod\n    def update_configs(cls, proj_dir: str, **kwargs) -&gt; ConfigParamsModel:\n        \"\"\"\n        If config_params file does not exist, makes a new one.\n\n        Then updates the config_params file with the kwargs.\n        If there are no kwargs, will not update the file\n        (other than making it if it did not exist).\n\n        Also creates all the project sub-directories too.\n\n        Finally, returns the ConfigParamsModel object.\n        \"\"\"\n        logger = init_logger_file()\n        pfm = ProjFpModel(proj_dir)\n        logger.debug(\"Making all the project sub-directories\")\n        logger.debug(\"Reading/creating params json\")\n        try:\n            configs = ConfigParamsModel.read_fp(pfm.config_params)\n            logger.debug(\"The configs file exists so using this file.\")\n        except FileNotFoundError:\n            logger.debug(\"The configs file does NOT exists.\")\n            configs = ConfigParamsModel()\n            logger.debug(\"Saving newly created configs file.\")\n            write_json(pfm.config_params, configs.model_dump())\n        if kwargs != {}:\n            logger.debug(f\"kwargs is not empty. They are: {kwargs}\")\n            configs_new = configs.model_validate(configs.model_copy(update=kwargs))\n            if configs_new != configs:\n                logger.debug(\"New configs are different from old configs. Overwriting to file.\")\n                write_json(pfm.config_params, configs_new.model_dump())\n        logger.debug(\"Returning the configs file\")\n        return configs\n\n    ###################################################################################################\n    # CONVERT TIFF TO ZARR FUNCS\n    ###################################################################################################\n\n    @classmethod\n    def tiff2zarr(cls, proj_dir: str, in_fp: str, overwrite: bool = False) -&gt; None:\n        \"\"\"\n        _summary_\n\n        Parameters\n        ----------\n        pfm : ProjFpModel\n            _description_\n        in_fp : str\n            _description_\n        overwrite : bool, optional\n            _description_, by default False\n\n        Raises\n        ------\n        ValueError\n            _description_\n        \"\"\"\n        logger = init_logger_file()\n        pfm = ProjFpModel(proj_dir)\n        if not overwrite:\n            for fp in (pfm.raw,):\n                if os.path.exists(fp):\n                    return logger.warning(file_exists_msg(fp))\n        logger.debug(\"Reading config params\")\n        configs = ConfigParamsModel.read_fp(pfm.config_params)\n        logger.debug(\"Making zarr from tiff file(s)\")\n        with cluster_process(LocalCluster(n_workers=1, threads_per_worker=6)):  # TODO: is this faster without cluster?\n            if os.path.isdir(in_fp):\n                logger.debug(f\"in_fp ({in_fp}) is a directory\")\n                logger.debug(\"Making zarr from tiff file stack in directory\")\n                Tiff2ZarrFuncs.tiffs2zarr(\n                    in_fp_ls=tuple(\n                        natsorted((os.path.join(in_fp, i) for i in os.listdir(in_fp) if re.search(r\".tif$\", i)))\n                    ),\n                    out_fp=pfm.raw,\n                    chunks=configs.zarr_chunksize,\n                )\n            elif os.path.isfile(in_fp):\n                logger.debug(f\"in_fp ({in_fp}) is a file\")\n                logger.debug(\"Making zarr from big-tiff file\")\n                Tiff2ZarrFuncs.btiff2zarr(\n                    in_fp=in_fp,\n                    out_fp=pfm.raw,\n                    chunks=configs.zarr_chunksize,\n                )\n            else:\n                raise ValueError(f'Input file path, \"{in_fp}\" does not exist.')\n\n    ###################################################################################################\n    # REGISTRATION PIPELINE FUNCS\n    ###################################################################################################\n\n    @classmethod\n    def reg_ref_prepare(cls, proj_dir: str, overwrite: bool = False) -&gt; None:\n        logger = init_logger_file()\n        pfm = ProjFpModel(proj_dir)\n        if not overwrite:\n            for fp in (pfm.ref, pfm.annot, pfm.map, pfm.affine, pfm.bspline):\n                if os.path.exists(fp):\n                    return logger.warning(file_exists_msg(fp))\n        # Getting configs\n        configs = ConfigParamsModel.read_fp(pfm.config_params)\n        # Making ref_fp_model of original atlas images filepaths\n        rfm = RefFpModel(\n            configs.atlas_dir,\n            configs.ref_version,\n            configs.annot_version,\n            configs.map_version,\n        )\n        # Making atlas images\n        for fp_i, fp_o in [\n            (rfm.ref, pfm.ref),\n            (rfm.annot, pfm.annot),\n        ]:\n            # Reading\n            arr = tifffile.imread(fp_i)\n            # Reorienting\n            arr = RegFuncs.reorient(arr, configs.ref_orient_ls)\n            # Slicing\n            arr = arr[\n                slice(*configs.ref_z_trim),\n                slice(*configs.ref_y_trim),\n                slice(*configs.ref_x_trim),\n            ]\n            # Saving\n            write_tiff(arr, fp_o)\n        # Copying region mapping json to project folder\n        shutil.copyfile(rfm.map, pfm.map)\n        # Copying transformation files\n        shutil.copyfile(rfm.affine, pfm.affine)\n        shutil.copyfile(rfm.bspline, pfm.bspline)\n\n    @classmethod\n    def reg_img_rough(cls, proj_dir: str, overwrite: bool = False) -&gt; None:\n        logger = init_logger_file()\n        pfm = ProjFpModel(proj_dir)\n        if not overwrite and os.path.exists(pfm.downsmpl1):\n            return logger.warning(file_exists_msg(pfm.downsmpl1))\n        # Getting configs\n        configs = ConfigParamsModel.read_fp(pfm.config_params)\n        with cluster_process(cls.busy_cluster()):  # TODO:  is this faster without cluster?\n            # Reading\n            raw_arr = da.from_zarr(pfm.raw)\n            # Rough downsample\n            downsmpl1_arr = RegFuncs.downsmpl_rough(raw_arr, configs.z_rough, configs.y_rough, configs.x_rough)\n            # Computing (from dask array)\n            downsmpl1_arr = downsmpl1_arr.compute()\n            # Saving\n            write_tiff(downsmpl1_arr, pfm.downsmpl1)\n\n    @classmethod\n    def reg_img_fine(cls, proj_dir: str, overwrite: bool = False) -&gt; None:\n        logger = init_logger_file()\n        pfm = ProjFpModel(proj_dir)\n        if not overwrite:\n            for fp in (pfm.downsmpl2,):\n                if os.path.exists(fp):\n                    return logger.warning(file_exists_msg(fp))\n        # Getting configs\n        configs = ConfigParamsModel.read_fp(pfm.config_params)\n        # Reading\n        downsmpl1_arr = tifffile.imread(pfm.downsmpl1)\n        # Fine downsample\n        downsmpl2_arr = RegFuncs.downsmpl_fine(downsmpl1_arr, configs.z_fine, configs.y_fine, configs.x_fine)\n        # Saving\n        write_tiff(downsmpl2_arr, pfm.downsmpl2)\n\n    @classmethod\n    def reg_img_trim(cls, proj_dir: str, overwrite: bool = False) -&gt; None:\n        logger = init_logger_file()\n        pfm = ProjFpModel(proj_dir)\n        if not overwrite:\n            for fp in (pfm.trimmed,):\n                if os.path.exists(fp):\n                    return logger.warning(file_exists_msg(fp))\n        # Getting configs\n        configs = ConfigParamsModel.read_fp(pfm.config_params)\n        # Reading\n        downsmpl2_arr = tifffile.imread(pfm.downsmpl2)\n        # Trim\n        trimmed_arr = downsmpl2_arr[\n            slice(*configs.z_trim),\n            slice(*configs.y_trim),\n            slice(*configs.x_trim),\n        ]\n        # Saving\n        write_tiff(trimmed_arr, pfm.trimmed)\n\n    @classmethod\n    def reg_img_bound(cls, proj_dir: str, overwrite: bool = False) -&gt; None:\n        logger = init_logger_file()\n        pfm = ProjFpModel(proj_dir)\n        if not overwrite:\n            for fp in (pfm.bounded,):\n                if os.path.exists(fp):\n                    return logger.warning(file_exists_msg(fp))\n        # Getting configs\n        configs = ConfigParamsModel.read_fp(pfm.config_params)\n        # Asserting that lower bound is less than upper bound\n        assert configs.lower_bound[0] &lt; configs.upper_bound[0], (\n            \"Error in config parameters: \" \"lower bound condition must be less than upper bound condition.\"\n        )\n        assert configs.lower_bound[1] &lt;= configs.lower_bound[0], (\n            \"Error in config parameters: \"\n            \"lower bound final value must be less than or equal to lower bound condition.\"\n        )\n        assert configs.upper_bound[1] &gt;= configs.upper_bound[0], (\n            \"Error in config parameters: \"\n            \"upper bound final value must be greater than or equal to upper bound condition.\"\n        )\n        # Reading\n        trimmed_arr = tifffile.imread(pfm.trimmed)\n        bounded_arr = trimmed_arr\n        # Bounding lower\n        bounded_arr[bounded_arr &lt; configs.lower_bound[0]] = configs.lower_bound[1]\n        # Bounding upper\n        bounded_arr[bounded_arr &gt; configs.upper_bound[0]] = configs.upper_bound[1]\n        # Saving\n        write_tiff(bounded_arr, pfm.bounded)\n\n    @classmethod\n    def reg_elastix(cls, proj_dir: str, overwrite: bool = False) -&gt; None:\n        logger = init_logger_file()\n        pfm = ProjFpModel(proj_dir)\n        if not overwrite:\n            for fp in (pfm.regresult,):\n                if os.path.exists(fp):\n                    return logger.warning(file_exists_msg(fp))\n        # Running Elastix registration\n        ElastixFuncs.registration(\n            fixed_img_fp=pfm.bounded,\n            moving_img_fp=pfm.ref,\n            output_img_fp=pfm.regresult,\n            affine_fp=pfm.affine,\n            bspline_fp=pfm.bspline,\n        )\n\n    ###################################################################################################\n    # MASK PIPELINE FUNCS\n    ###################################################################################################\n\n    @classmethod\n    def make_mask(cls, proj_dir: str, overwrite: bool = False) -&gt; None:\n        \"\"\"\n        Makes mask of actual image in reference space.\n        Also stores # and proportion of existent voxels\n        for each region.\n        \"\"\"\n        logger = init_logger_file()\n        pfm = ProjFpModel(proj_dir)\n        if not overwrite:\n            for fp in (pfm.mask_fill, pfm.mask_outline, pfm.mask_reg, pfm.mask_df):\n                if os.path.exists(fp):\n                    return logger.warning(file_exists_msg(fp))\n        # Getting configs\n        configs = ConfigParamsModel.read_fp(pfm.config_params)\n        # Reading annot img (proj oriented and trimmed) and bounded img\n        annot_arr = tifffile.imread(pfm.annot)\n        bounded_arr = tifffile.imread(pfm.bounded)\n        # Storing annot_arr shape\n        s = annot_arr.shape\n        # Making mask\n        blur_arr = cls.cellc_funcs.gauss_blur_filt(bounded_arr, configs.mask_gaus_blur)\n        write_tiff(blur_arr, pfm.premask_blur)\n        mask_arr = cls.cellc_funcs.manual_thresh(blur_arr, configs.mask_thresh)\n        write_tiff(mask_arr, pfm.mask_fill)\n\n        # Make outline\n        outline_df = MaskFuncs.make_outline(mask_arr)\n        # Transformix on coords\n        outline_df[[Coords.Z.value, Coords.Y.value, Coords.X.value]] = (\n            ElastixFuncs.transformation_coords(\n                outline_df,\n                pfm.ref,\n                pfm.regresult,\n            )[[Coords.Z.value, Coords.Y.value, Coords.X.value]]\n            .round(0)\n            .astype(np.int32)\n        )\n        # Filtering out of bounds coords\n        outline_df = outline_df.query(\n            f\"({Coords.Z.value} &gt;= 0) &amp; ({Coords.Z.value} &lt; {s[0]}) &amp; \"\n            f\"({Coords.Y.value} &gt;= 0) &amp; ({Coords.Y.value} &lt; {s[1]}) &amp; \"\n            f\"({Coords.X.value} &gt;= 0) &amp; ({Coords.X.value} &lt; {s[2]})\"\n        )\n\n        # Make outline img (1 for in, 2 for out)\n        # TODO: convert to return np.array and save out-of-function\n        VisualCheckFuncsTiff.coords2points(outline_df[outline_df.is_in == 1], s, pfm.mask_outline)\n        in_arr = tifffile.imread(pfm.mask_outline)\n        VisualCheckFuncsTiff.coords2points(outline_df[outline_df.is_in == 0], s, pfm.mask_outline)\n        out_arr = tifffile.imread(pfm.mask_outline)\n        write_tiff(in_arr + out_arr * 2, pfm.mask_outline)\n\n        # Fill in outline to recreate mask (not perfect)\n        mask_reg_arr = MaskFuncs.fill_outline(outline_df, s)\n        # Opening (removes FP) and closing (fills FN)\n        mask_reg_arr = ndimage.binary_closing(mask_reg_arr, iterations=2).astype(np.uint8)\n        mask_reg_arr = ndimage.binary_opening(mask_reg_arr, iterations=2).astype(np.uint8)\n        # Saving\n        write_tiff(mask_reg_arr, pfm.mask_reg)\n\n        # Counting mask voxels in each region\n        # Getting original annot fp by making ref_fp_model\n        rfm = RefFpModel(\n            configs.atlas_dir,\n            configs.ref_version,\n            configs.annot_version,\n            configs.map_version,\n        )\n        # Reading original annot\n        annot_orig_arr = tifffile.imread(rfm.annot)\n        # Getting the annotation name for every cell (zyx coord)\n        mask_df = pd.merge(\n            left=MaskFuncs.mask2region_counts(np.full(annot_orig_arr.shape, 1), annot_orig_arr),\n            right=MaskFuncs.mask2region_counts(mask_reg_arr, annot_arr),\n            how=\"left\",\n            left_index=True,\n            right_index=True,\n            suffixes=(\"_annot\", \"_mask\"),\n        ).fillna(0)\n        # Reading annotation mappings json\n        annot_df = MapFuncs.annot_dict2df(read_json(pfm.map))\n        # Combining (summing) the mask_df volumes for parent regions using the annot_df\n        mask_df = MapFuncs.combine_nested_regions(mask_df, annot_df)\n        # Calculating proportion of mask volume in each region\n        mask_df[MaskColumns.VOLUME_PROP.value] = (\n            mask_df[MaskColumns.VOLUME_MASK.value] / mask_df[MaskColumns.VOLUME_ANNOT.value]\n        )\n        # Selecting and ordering relevant columns\n        mask_df = mask_df[[*ANNOT_COLUMNS_FINAL, *enum2list(MaskColumns)]]\n        # Saving\n        write_parquet(mask_df, pfm.mask_df)\n\n    ###################################################################################################\n    # CROP RAW ZARR TO MAKE TUNING ZARR\n    ###################################################################################################\n\n    @classmethod\n    def make_tuning_arr(cls, proj_dir: str, overwrite: bool = False) -&gt; None:\n        \"\"\"\n        Crop raw zarr to make a smaller zarr for tuning the cell counting pipeline.\n        \"\"\"\n        logger = init_logger_file()\n        logger.debug(\"Converting/ensuring pfm is production filepaths (copy)\")\n        pfm = ProjFpModel(proj_dir)\n        pfm_tuning = ProjFpModelTuning(proj_dir)\n        logger.debug(\"Reading config params\")\n        configs = ConfigParamsModel.read_fp(pfm.config_params)\n        with cluster_process(cls.busy_cluster()):  # TODO:  is this faster without cluster?\n            logger.debug(\"Reading raw zarr\")\n            raw_arr = da.from_zarr(pfm.raw)\n            logger.debug(\"Cropping raw zarr\")\n            raw_arr = raw_arr[\n                slice(*configs.tuning_z_trim),\n                slice(*configs.tuning_y_trim),\n                slice(*configs.tuning_x_trim),\n            ]\n            if not overwrite:\n                for fp in (pfm_tuning.raw,):\n                    if os.path.exists(fp):\n                        return logger.warning(file_exists_msg(fp))\n            logger.debug(\"Saving cropped raw zarr\")\n            raw_arr = disk_cache(raw_arr, pfm_tuning.raw)\n\n    ###################################################################################################\n    # CELL COUNTING PIPELINE FUNCS\n    ###################################################################################################\n\n    @classmethod\n    def img_overlap(cls, proj_dir: str, overwrite: bool = False, tuning: bool = False) -&gt; None:\n        logger = init_logger_file()\n        pfm = ProjFpModelTuning(proj_dir) if tuning else ProjFpModel(proj_dir)\n        if not overwrite:\n            for fp in (pfm.overlap,):\n                if os.path.exists(fp):\n                    return logger.warning(file_exists_msg(fp))\n        # Getting configs\n        configs = ConfigParamsModel.read_fp(pfm.config_params)\n        # Making overlap image\n        with cluster_process(cls.heavy_cluster()):  # TODO: is this faster without cluster?\n            raw_arr = da.from_zarr(pfm.raw, chunks=configs.zarr_chunksize)\n            overlap_arr = da_overlap(raw_arr, d=configs.overlap_depth)\n            overlap_arr = disk_cache(overlap_arr, pfm.overlap)\n\n    @classmethod\n    def cellc1(cls, proj_dir: str, overwrite: bool = False, tuning: bool = False) -&gt; None:\n        \"\"\"\n        Cell counting pipeline - Step 1\n\n        Top-hat filter (background subtraction)\n        \"\"\"\n        logger = init_logger_file()\n        pfm = ProjFpModelTuning(proj_dir) if tuning else ProjFpModel(proj_dir)\n        if not overwrite:\n            for fp in (pfm.bgrm,):\n                if os.path.exists(fp):\n                    return logger.warning(file_exists_msg(fp))\n        # Making Dask cluster\n        with cluster_process(cls.gpu_cluster()):\n            # Getting configs\n            configs = ConfigParamsModel.read_fp(pfm.config_params)\n            # Reading input images\n            overlap_arr = da.from_zarr(pfm.overlap)\n            # Declaring processing instructions\n            bgrm_arr = da.map_blocks(\n                cls.cellc_funcs.tophat_filt,\n                overlap_arr,\n                configs.tophat_sigma,\n            )\n            # Computing and saving\n            bgrm_arr = disk_cache(bgrm_arr, pfm.bgrm)\n\n    @classmethod\n    def cellc2(cls, proj_dir: str, overwrite: bool = False, tuning: bool = False) -&gt; None:\n        \"\"\"\n        Cell counting pipeline - Step 2\n\n        Difference of Gaussians (edge detection)\n        \"\"\"\n        logger = init_logger_file()\n        pfm = ProjFpModelTuning(proj_dir) if tuning else ProjFpModel(proj_dir)\n        if not overwrite:\n            for fp in (pfm.dog,):\n                if os.path.exists(fp):\n                    return logger.warning(file_exists_msg(fp))\n        # Making Dask cluster\n        with cluster_process(cls.gpu_cluster()):\n            # Getting configs\n            configs = ConfigParamsModel.read_fp(pfm.config_params)\n            # Reading input images\n            bgrm_arr = da.from_zarr(pfm.bgrm)\n            # Declaring processing instructions\n            dog_arr = da.map_blocks(\n                cls.cellc_funcs.dog_filt,\n                bgrm_arr,\n                configs.dog_sigma1,\n                configs.dog_sigma2,\n            )\n            # Computing and saving\n            dog_arr = disk_cache(dog_arr, pfm.dog)\n\n    @classmethod\n    def cellc3(cls, proj_dir: str, overwrite: bool = False, tuning: bool = False) -&gt; None:\n        \"\"\"\n        Cell counting pipeline - Step 3\n\n        Gaussian subtraction with large sigma for adaptive thresholding\n        \"\"\"\n        logger = init_logger_file()\n        pfm = ProjFpModelTuning(proj_dir) if tuning else ProjFpModel(proj_dir)\n        if not overwrite:\n            for fp in (pfm.adaptv,):\n                if os.path.exists(fp):\n                    return logger.warning(file_exists_msg(fp))\n        # Making Dask cluster\n        with cluster_process(cls.gpu_cluster()):\n            # Getting configs\n            configs = ConfigParamsModel.read_fp(pfm.config_params)\n            # Reading input images\n            dog_arr = da.from_zarr(pfm.dog)\n            # Declaring processing instructions\n            adaptv_arr = da.map_blocks(\n                cls.cellc_funcs.gauss_subt_filt,\n                dog_arr,\n                configs.large_gauss_sigma,\n            )\n            # Computing and saving\n            adaptv_arr = disk_cache(adaptv_arr, pfm.adaptv)\n\n    @classmethod\n    def cellc4(cls, proj_dir: str, overwrite: bool = False, tuning: bool = False) -&gt; None:\n        \"\"\"\n        Cell counting pipeline - Step 4\n\n        Currently, manual thresholding.\n        Ideally, mean thresholding with standard deviation offset\n        \"\"\"\n        logger = init_logger_file()\n        pfm = ProjFpModelTuning(proj_dir) if tuning else ProjFpModel(proj_dir)\n        if not overwrite:\n            for fp in (pfm.threshd,):\n                if os.path.exists(fp):\n                    return logger.warning(file_exists_msg(fp))\n        # Making Dask cluster\n        with cluster_process(cls.gpu_cluster()):\n            # Getting configs\n            configs = ConfigParamsModel.read_fp(pfm.config_params)\n            # # Visually inspect sd offset\n            # t_p =adaptv_arr.sum() / (np.prod(adaptv_arr.shape) - (adaptv_arr == 0).sum())\n            # t_p = t_p.compute()\n            # Reading input images\n            adaptv_arr = da.from_zarr(pfm.adaptv)\n            # Declaring processing instructions\n            threshd_arr = da.map_blocks(\n                cls.cellc_funcs.manual_thresh,  # NOTE: previously CPU\n                adaptv_arr,\n                configs.threshd_value,\n            )\n            # Computing and saving\n            threshd_arr = disk_cache(threshd_arr, pfm.threshd)\n\n    @classmethod\n    def cellc5(cls, proj_dir: str, overwrite: bool = False, tuning: bool = False) -&gt; None:\n        \"\"\"\n        Cell counting pipeline - Step 5\n\n        Getting object sizes\n        \"\"\"\n        logger = init_logger_file()\n        pfm = ProjFpModelTuning(proj_dir) if tuning else ProjFpModel(proj_dir)\n        if not overwrite:\n            for fp in (pfm.threshd_volumes,):\n                if os.path.exists(fp):\n                    return logger.warning(file_exists_msg(fp))\n        # Making Dask cluster\n        with cluster_process(cls.gpu_cluster()):\n            # Reading input images\n            threshd_arr = da.from_zarr(pfm.threshd)\n            # Declaring processing instructions\n            threshd_volumes_arr = da.map_blocks(\n                cls.cellc_funcs.mask2volume,  # NOTE: previously CPU\n                threshd_arr,\n            )\n            # Computing and saving\n            threshd_volumes_arr = disk_cache(threshd_volumes_arr, pfm.threshd_volumes)\n\n    @classmethod\n    def cellc6(cls, proj_dir: str, overwrite: bool = False, tuning: bool = False) -&gt; None:\n        \"\"\"\n        Cell counting pipeline - Step 6\n\n        Filter out large objects (likely outlines, not cells)\n        \"\"\"\n        logger = init_logger_file()\n        pfm = ProjFpModelTuning(proj_dir) if tuning else ProjFpModel(proj_dir)\n        if not overwrite:\n            for fp in (pfm.threshd_filt,):\n                if os.path.exists(fp):\n                    return logger.warning(file_exists_msg(fp))\n        with cluster_process(cls.gpu_cluster()):\n            # Getting configs\n            configs = ConfigParamsModel.read_fp(pfm.config_params)\n            # Reading input images\n            threshd_volumes_arr = da.from_zarr(pfm.threshd_volumes)\n            # Declaring processing instructions\n            threshd_filt_arr = da.map_blocks(\n                cls.cellc_funcs.volume_filter,\n                threshd_volumes_arr,\n                configs.min_threshd_size,\n                configs.max_threshd_size,\n            )\n            # Computing and saving\n            threshd_filt_arr = disk_cache(threshd_filt_arr, pfm.threshd_filt)\n\n    @classmethod\n    def cellc7(cls, proj_dir: str, overwrite: bool = False, tuning: bool = False) -&gt; None:\n        \"\"\"\n        Cell counting pipeline - Step 7\n\n        Get maxima of image masked by labels.\n        \"\"\"\n        logger = init_logger_file()\n        pfm = ProjFpModelTuning(proj_dir) if tuning else ProjFpModel(proj_dir)\n        if not overwrite:\n            for fp in (pfm.maxima,):\n                if os.path.exists(fp):\n                    return logger.warning(file_exists_msg(fp))\n        with cluster_process(cls.gpu_cluster()):\n            # Getting configs\n            configs = ConfigParamsModel.read_fp(pfm.config_params)\n            # Reading input images\n            overlap_arr = da.from_zarr(pfm.overlap)\n            threshd_filt_arr = da.from_zarr(pfm.threshd_filt)\n            # Declaring processing instructions\n            maxima_arr = da.map_blocks(\n                cls.cellc_funcs.get_local_maxima,\n                overlap_arr,\n                configs.maxima_sigma,\n                threshd_filt_arr,\n            )\n            # Computing and saving\n            maxima_arr = disk_cache(maxima_arr, pfm.maxima)\n\n    @classmethod\n    def cellc7b(cls, proj_dir: str, overwrite: bool = False, tuning: bool = False) -&gt; None:\n        \"\"\"\n        Cell counting pipeline - Step 7\n\n        Convert maxima mask to uniquely labelled points.\n        \"\"\"\n        # TODO: Check that the results of cellc10 and cellc7b, cellc8a, cellc10a are the same (df)\n        logger = init_logger_file()\n        pfm = ProjFpModelTuning(proj_dir) if tuning else ProjFpModel(proj_dir)\n        if not overwrite:\n            for fp in (pfm.maxima_labels,):\n                if os.path.exists(fp):\n                    return logger.warning(file_exists_msg(fp))\n        with cluster_process(cls.gpu_cluster()):\n            # Reading input images\n            maxima_arr = da.from_zarr(pfm.maxima)\n            # Declaring processing instructions\n            maxima_labels_arr = da.map_blocks(\n                cls.cellc_funcs.mask2label,\n                maxima_arr,\n            )\n            maxima_labels_arr = disk_cache(maxima_labels_arr, pfm.maxima_labels)\n\n    # NOTE: NOT NEEDED TO GET WSHED_LABELS AS ALL WE NEED IS WSBEED_VOLUMES FOR CELLC10b\n    # @classmethod\n    # def cellc8a(cls, proj_dir: str, overwrite: bool = False, tuning: bool = False) -&gt; None:\n    #     \"\"\"\n    #     Cell counting pipeline - Step 8\n\n    #     Watershed segmentation labels.\n    #     \"\"\"\n    #     logger = init_logger_file()\n    #     pfm = ProjFpModelTuning(proj_dir) if tuning else ProjFpModel(proj_dir)\n    #     if not overwrite:\n    #         for fp in (pfm.maxima_labels.val,):\n    #             if os.path.exists(fp):\n    #                 return logger.warning(file_exists_msg(fp))\n    #     with cluster_process(cls.heavy_cluster()):\n    #         # Reading input images\n    #         overlap_arr = da.from_zarr(pfm.overlap.val)\n    #         maxima_labels_arr = da.from_zarr(pfm.maxima_labels.val)\n    #         threshd_filt_arr = da.from_zarr(pfm.threshd_filt.val)\n    #         # Declaring processing instructions\n    #         wshed_labels_arr = da.map_blocks(\n    #             cls.cellc_funcs.wshed_segm,\n    #             overlap_arr,\n    #             maxima_labels_arr,\n    #             threshd_filt_arr,\n    #         )\n    #         wshed_labels_arr = disk_cache(wshed_labels_arr, pfm.wshed_labels.val)\n\n    # @classmethod\n    # def cellc8b(cls, proj_dir: str, overwrite: bool = False) -&gt; None:\n    #     \"\"\"\n    #     Cell counting pipeline - Step 8\n\n    #     Watershed segmentation volumes.\n    #     \"\"\"\n    #     logger = init_logger_file()\n    #     if not overwrite:\n    #         for fp in (pfm.maxima_labels.val,):\n    #             if os.path.exists(fp):\n    #                 return logger.warning(file_exists_msg(fp))\n    #     with cluster_process(cls.heavy_cluster()):\n    #         # Reading input images\n    #         wshed_labels_arr = da.from_zarr(pfm.wshed_labels.val)\n    #         # Declaring processing instructions\n    #         wshed_volumes_arr = da.map_blocks(\n    #             cls.cellc_funcs.label2volume,\n    #             wshed_labels_arr,\n    #         )\n    #         wshed_volumes_arr = disk_cache(wshed_volumes_arr, pfm.wshed_volumes.val)\n\n    @classmethod\n    def cellc8(cls, proj_dir: str, overwrite: bool = False, tuning: bool = False) -&gt; None:\n        \"\"\"\n        Cell counting pipeline - Step 8\n\n        Watershed segmentation volumes.\n        \"\"\"\n        logger = init_logger_file()\n        pfm = ProjFpModelTuning(proj_dir) if tuning else ProjFpModel(proj_dir)\n        if not overwrite:\n            for fp in (pfm.wshed_volumes,):\n                if os.path.exists(fp):\n                    return logger.warning(file_exists_msg(fp))\n        with cluster_process(cls.heavy_cluster()):\n            # n_workers=2\n            # Reading input images\n            overlap_arr = da.from_zarr(pfm.overlap)\n            maxima_arr = da.from_zarr(pfm.maxima)\n            threshd_filt_arr = da.from_zarr(pfm.threshd_filt)\n            # Declaring processing instructions\n            wshed_volumes_arr = da.map_blocks(\n                CpuCellcFuncs.wshed_segm_volumes,\n                overlap_arr,\n                maxima_arr,\n                threshd_filt_arr,\n            )\n            wshed_volumes_arr = disk_cache(wshed_volumes_arr, pfm.wshed_volumes)\n\n    @classmethod\n    def cellc9(cls, proj_dir: str, overwrite: bool = False, tuning: bool = False) -&gt; None:\n        \"\"\"\n        Cell counting pipeline - Step 9\n\n        Filter out large watershed objects (again cell areas, not cells).\n        \"\"\"\n        logger = init_logger_file()\n        pfm = ProjFpModelTuning(proj_dir) if tuning else ProjFpModel(proj_dir)\n        if not overwrite:\n            for fp in (pfm.wshed_filt,):\n                if os.path.exists(fp):\n                    return logger.warning(file_exists_msg(fp))\n        with cluster_process(cls.gpu_cluster()):\n            # Getting configs\n            configs = ConfigParamsModel.read_fp(pfm.config_params)\n            # Reading input images\n            wshed_volumes_arr = da.from_zarr(pfm.wshed_volumes)\n            # Declaring processing instructions\n            wshed_filt_arr = da.map_blocks(\n                cls.cellc_funcs.volume_filter,\n                wshed_volumes_arr,\n                configs.min_wshed_size,\n                configs.max_wshed_size,\n            )\n            # Computing and saving\n            wshed_filt_arr = disk_cache(wshed_filt_arr, pfm.wshed_filt)\n\n    @classmethod\n    def cellc10(cls, proj_dir: str, overwrite: bool = False, tuning: bool = False) -&gt; None:\n        \"\"\"\n        Cell counting pipeline - Step 11\n\n        Calculate the maxima and watershed, save the cells.\n\n        Basically a repeat of cellc8 and cellc9 but needs to be done to\n        get the cell volumes in a table. Hence, don't run cellc8 and cellc9 if\n        you don't want to view the cells visually (good for pipeline, not for tuning).\n        \"\"\"\n        logger = init_logger_file()\n        pfm = ProjFpModelTuning(proj_dir) if tuning else ProjFpModel(proj_dir)\n        if not overwrite:\n            for fp in (pfm.cells_raw_df,):\n                if os.path.exists(fp):\n                    return logger.warning(file_exists_msg(fp))\n        with cluster_process(cls.heavy_cluster()):\n            # Getting configs\n            configs = ConfigParamsModel.read_fp(pfm.config_params)\n            # Reading input images\n            raw_arr = da.from_zarr(pfm.raw)\n            overlap_arr = da.from_zarr(pfm.overlap)\n            maxima_arr = da.from_zarr(pfm.maxima)\n            threshd_filt_arr = da.from_zarr(pfm.threshd_filt)\n            # Declaring processing instructions\n            # Getting maxima coords and cell measures in table\n            cells_df = block2coords(\n                CpuCellcFuncs.get_cells,\n                raw_arr,\n                overlap_arr,\n                maxima_arr,\n                threshd_filt_arr,\n                configs.overlap_depth,\n            )\n            # Converting from dask to pandas\n            cells_df = cells_df.compute()\n            # Filtering out by volume (same filter cellc9_pipeline volume_filter)\n            cells_df = cells_df.query(\n                f\"({CellColumns.VOLUME.value} &gt;= {configs.min_wshed_size}) &amp; \"\n                f\"({CellColumns.VOLUME.value} &lt;= {configs.max_wshed_size})\"\n            )\n            # Computing and saving as parquet\n            write_parquet(cells_df, pfm.cells_raw_df)\n\n    @classmethod\n    def cellc10b(cls, proj_dir: str, overwrite: bool = False, tuning: bool = False) -&gt; None:\n        \"\"\"\n        Alternative to cellc10.\n\n        Uses raw, overlap, maxima_labels, wshed_filt (so wshed computation not run again).\n        Also allows GPU processing.\n        \"\"\"\n        logger = init_logger_file()\n        pfm = ProjFpModelTuning(proj_dir) if tuning else ProjFpModel(proj_dir)\n        if not overwrite:\n            for fp in (pfm.cells_raw_df,):\n                if os.path.exists(fp):\n                    return logger.warning(file_exists_msg(fp))\n        with cluster_process(cls.heavy_cluster()):\n            # Getting configs\n            configs = ConfigParamsModel.read_fp(pfm.config_params)\n            # Reading input images\n            raw_arr = da.from_zarr(pfm.raw)\n            overlap_arr = da.from_zarr(pfm.overlap)\n            maxima_labels_arr = da.from_zarr(pfm.maxima_labels)\n            wshed_labels_arr = da.from_zarr(pfm.wshed_labels)\n            wshed_filt_arr = da.from_zarr(pfm.wshed_filt)\n            # Declaring processing instructions\n            # Getting maxima coords and cell measures in table\n            cells_df = block2coords(\n                CpuCellcFuncs.get_cells_b,\n                raw_arr,\n                overlap_arr,\n                maxima_labels_arr,\n                wshed_labels_arr,\n                wshed_filt_arr,\n                configs.overlap_depth,\n            )\n            # Converting from dask to pandas\n            cells_df = cells_df.compute()\n            # Computing and saving as parquet\n            write_parquet(cells_df, pfm.cells_raw_df)\n\n    ###################################################################################################\n    # CELL COUNT REALIGNMENT TO REFERENCE AND AGGREGATION PIPELINE FUNCS\n    ###################################################################################################\n\n    @classmethod\n    def transform_coords(cls, proj_dir: str, overwrite: bool = False) -&gt; None:\n        \"\"\"\n        `in_id` and `out_id` are either maxima or region\n\n        NOTE: saves the cells_trfm dataframe as pandas parquet.\n        \"\"\"\n        logger = init_logger_file()\n        pfm = ProjFpModel(proj_dir)\n        if not overwrite:\n            for fp in (pfm.cells_trfm_df,):\n                if os.path.exists(fp):\n                    return logger.warning(file_exists_msg(fp))\n        # Getting configs\n        configs = ConfigParamsModel.read_fp(pfm.config_params)\n        with cluster_process(cls.busy_cluster()):\n            # Setting output key (in the form \"&lt;maxima/region&gt;_trfm_df\")\n            # Getting cell coords\n            cells_df = pd.read_parquet(pfm.cells_raw_df)\n            # Sanitising (removing smb columns)\n            cells_df = sanitise_smb_df(cells_df)\n            # Taking only Coords.Z.value, Coords.Y.value, Coords.X.value coord columns\n            cells_df = cells_df[enum2list(Coords)]\n            # Scaling to resampled rough space\n            # NOTE: this downsampling uses slicing so must be computed differently\n            cells_df = cells_df / np.array((configs.z_rough, configs.y_rough, configs.x_rough))\n            # Scaling to resampled space\n            cells_df = cells_df * np.array((configs.z_fine, configs.y_fine, configs.x_fine))\n            # Trimming/offsetting to sliced space\n            cells_df = cells_df - np.array([s[0] or 0 for s in (configs.z_trim, configs.y_trim, configs.x_trim)])\n            # Converting back to DataFrame\n            cells_df = pd.DataFrame(cells_df, columns=enum2list(Coords))\n\n            cells_trfm_df = ElastixFuncs.transformation_coords(cells_df, pfm.ref, pfm.regresult)\n            # NOTE: Using pandas parquet. does not work with dask yet\n            # cells_df = dd.from_pandas(cells_df, npartitions=1)\n            # Fitting resampled space to atlas image with Transformix (from Elastix registration step)\n            # cells_df = cells_df.repartition(\n            #     npartitions=int(np.ceil(cells_df.shape[0].compute() / ROWSPPART))\n            # )\n            # cells_df = cells_df.map_partitions(\n            #     ElastixFuncs.transformation_coords, pfm.ref.val, pfm.regresult.val\n            # )\n            write_parquet(cells_trfm_df, pfm.cells_trfm_df)\n\n    @classmethod\n    def cell_mapping(cls, proj_dir: str, overwrite: bool = False) -&gt; None:\n        \"\"\"\n        Using the transformed cell coordinates, get the region ID and name for each cell\n        corresponding to the reference atlas.\n\n        NOTE: saves the cells dataframe as pandas parquet.\n        \"\"\"\n        logger = init_logger_file()\n        pfm = ProjFpModel(proj_dir)\n        if not overwrite:\n            for fp in (pfm.cells_df,):\n                if os.path.exists(fp):\n                    return logger.warning(file_exists_msg(fp))\n        # Getting region for each detected cell (i.e. row) in cells_df\n        with cluster_process(cls.busy_cluster()):\n            # Reading cells_raw and cells_trfm dataframes\n            cells_df = pd.read_parquet(pfm.cells_raw_df)\n            coords_trfm = pd.read_parquet(pfm.cells_trfm_df)\n            # Sanitising (removing smb columns)\n            cells_df = sanitise_smb_df(cells_df)\n            coords_trfm = sanitise_smb_df(coords_trfm)\n            # Making unique incrementing index\n            cells_df = cells_df.reset_index(drop=True)\n            # Setting the transformed coords\n            cells_df[f\"{Coords.Z.value}_{TRFM}\"] = coords_trfm[Coords.Z.value].values\n            cells_df[f\"{Coords.Y.value}_{TRFM}\"] = coords_trfm[Coords.Y.value].values\n            cells_df[f\"{Coords.X.value}_{TRFM}\"] = coords_trfm[Coords.X.value].values\n\n            # Reading annotation image\n            annot_arr = tifffile.imread(pfm.annot)\n            # Getting the annotation ID for every cell (zyx coord)\n            # Getting transformed coords (that are within tbe bounds_arr, and their corresponding idx)\n            s = annot_arr.shape\n            trfm_loc = (\n                cells_df[\n                    [\n                        f\"{Coords.Z.value}_{TRFM}\",\n                        f\"{Coords.Y.value}_{TRFM}\",\n                        f\"{Coords.X.value}_{TRFM}\",\n                    ]\n                ]\n                .round(0)\n                .astype(np.int32)\n                .query(\n                    f\"({Coords.Z.value}_{TRFM} &gt;= 0) &amp; ({Coords.Z.value}_{TRFM} &lt; {s[0]}) &amp; \"\n                    f\"({Coords.Y.value}_{TRFM} &gt;= 0) &amp; ({Coords.Y.value}_{TRFM} &lt; {s[1]}) &amp; \"\n                    f\"({Coords.X.value}_{TRFM} &gt;= 0) &amp; ({Coords.X.value}_{TRFM} &lt; {s[2]})\"\n                )\n            )\n            # Getting the pixel values of each valid transformed coord (hence the specified index)\n            # By complex array indexing on ar_annot's (z, y, x) dimensions.\n            # nulls are imputed with -1\n            cells_df[AnnotColumns.ID.value] = pd.Series(\n                annot_arr[*trfm_loc.values.T].astype(np.uint32),\n                index=trfm_loc.index,\n            ).fillna(-1)\n\n            # Reading annotation mappings dataframe\n            annot_df = MapFuncs.annot_dict2df(read_json(pfm.map))\n            # Getting the annotation name for every cell (zyx coord)\n            cells_df = MapFuncs.df_map_ids(cells_df, annot_df)\n            # Saving to disk\n            # NOTE: Using pandas parquet. does not work with dask yet\n            # cells_df = dd.from_pandas(cells_df)\n            write_parquet(cells_df, pfm.cells_df)\n\n    @classmethod\n    def group_cells(cls, proj_dir: str, overwrite: bool = False) -&gt; None:\n        \"\"\"\n        Grouping cells by region name and aggregating total cell volume\n        and cell count for each region.\n\n        NOTE: saves the cells_agg dataframe as pandas parquet.\n        \"\"\"\n        logger = init_logger_file()\n        pfm = ProjFpModel(proj_dir)\n        if not overwrite:\n            for fp in (pfm.cells_agg_df,):\n                if os.path.exists(fp):\n                    return logger.warning(file_exists_msg(fp))\n        # Making cells_agg_df\n        with cluster_process(cls.busy_cluster()):\n            # Reading cells dataframe\n            cells_df = pd.read_parquet(pfm.cells_df)\n            # Sanitising (removing smb columns)\n            cells_df = sanitise_smb_df(cells_df)\n            # Grouping cells by region name and aggregating on given mappings\n            cells_agg_df = cells_df.groupby(AnnotColumns.ID.value).agg(CELL_AGG_MAPPINGS)\n            cells_agg_df.columns = list(CELL_AGG_MAPPINGS.keys())\n            # Reading annotation mappings dataframe\n            # Making df of region names and their parent region names\n            annot_df = MapFuncs.annot_dict2df(read_json(pfm.map))\n            # Combining (summing) the cells_agg_df values for parent regions using the annot_df\n            cells_agg_df = MapFuncs.combine_nested_regions(cells_agg_df, annot_df)\n            # Calculating integrated average intensity (sum_intensity / volume)\n            cells_agg_df[CellColumns.IOV.value] = (\n                cells_agg_df[CellColumns.SUM_INTENSITY.value] / cells_agg_df[CellColumns.VOLUME.value]\n            )\n            # Selecting and ordering relevant columns\n            cells_agg_df = cells_agg_df[[*ANNOT_COLUMNS_FINAL, *enum2list(CellColumns)]]\n            # Saving to disk\n            # NOTE: Using pandas parquet. does not work with dask yet\n            # cells_agg = dd.from_pandas(cells_agg)\n            write_parquet(cells_agg_df, pfm.cells_agg_df)\n\n    @classmethod\n    def cells2csv(cls, proj_dir: str, overwrite: bool = False) -&gt; None:\n        logger = init_logger_file()\n        pfm = ProjFpModel(proj_dir)\n        if not overwrite:\n            for fp in (pfm.cells_agg_csv,):\n                if os.path.exists(fp):\n                    return logger.warning(file_exists_msg(fp))\n        # Reading cells dataframe\n        cells_agg_df = pd.read_parquet(pfm.cells_agg_df)\n        # Sanitising (removing smb columns)\n        cells_agg_df = sanitise_smb_df(cells_agg_df)\n        # Saving to csv\n        cells_agg_df.to_csv(pfm.cells_agg_csv)\n\n    ###################################################################################################\n    # ALL PIPELINE FUNCTION\n    ###################################################################################################\n\n    @classmethod\n    def run_pipeline(cls, in_fp: str, proj_dir: str, overwrite: bool = False, **kwargs) -&gt; None:\n        \"\"\"\n        Running all pipelines in order.\n        \"\"\"\n        # Updating project configs\n        cls.update_configs(proj_dir, **kwargs)\n        # tiff to zarr\n        cls.tiff2zarr(proj_dir, in_fp, overwrite=overwrite)\n        # Registration\n        cls.reg_ref_prepare(proj_dir, overwrite=overwrite)\n        cls.reg_img_rough(proj_dir, overwrite=overwrite)\n        cls.reg_img_fine(proj_dir, overwrite=overwrite)\n        cls.reg_img_trim(proj_dir, overwrite=overwrite)\n        cls.reg_img_bound(proj_dir, overwrite=overwrite)\n        cls.reg_elastix(proj_dir, overwrite=overwrite)\n        # Coverage mask\n        cls.make_mask(proj_dir, overwrite=overwrite)\n        # Making trimmed image for cell count tuning\n        cls.make_tuning_arr(proj_dir, overwrite=overwrite)\n        # Cell counting (tuning and final)\n        for is_tuning in [True, False]:\n            cls.img_overlap(proj_dir, overwrite=overwrite, tuning=is_tuning)\n            cls.cellc1(proj_dir, overwrite=overwrite, tuning=is_tuning)\n            cls.cellc2(proj_dir, overwrite=overwrite, tuning=is_tuning)\n            cls.cellc3(proj_dir, overwrite=overwrite, tuning=is_tuning)\n            cls.cellc4(proj_dir, overwrite=overwrite, tuning=is_tuning)\n            cls.cellc5(proj_dir, overwrite=overwrite, tuning=is_tuning)\n            cls.cellc6(proj_dir, overwrite=overwrite, tuning=is_tuning)\n            cls.cellc7(proj_dir, overwrite=overwrite, tuning=is_tuning)\n            cls.cellc8(proj_dir, overwrite=overwrite, tuning=is_tuning)\n            cls.cellc9(proj_dir, overwrite=overwrite, tuning=is_tuning)\n            cls.cellc10(proj_dir, overwrite=overwrite, tuning=is_tuning)\n        # Cell mapping\n        cls.transform_coords(proj_dir, overwrite=overwrite)\n        cls.cell_mapping(proj_dir, overwrite=overwrite)\n        cls.group_cells(proj_dir, overwrite=overwrite)\n        cls.cells2csv(proj_dir, overwrite=overwrite)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#cellcounter.pipeline.pipeline.Pipeline.cell_mapping","title":"<code>cell_mapping(proj_dir, overwrite=False)</code>  <code>classmethod</code>","text":"<p>Using the transformed cell coordinates, get the region ID and name for each cell corresponding to the reference atlas.</p> <p>NOTE: saves the cells dataframe as pandas parquet.</p> Source code in <code>cellcounter/pipeline/pipeline.py</code> <pre><code>@classmethod\ndef cell_mapping(cls, proj_dir: str, overwrite: bool = False) -&gt; None:\n    \"\"\"\n    Using the transformed cell coordinates, get the region ID and name for each cell\n    corresponding to the reference atlas.\n\n    NOTE: saves the cells dataframe as pandas parquet.\n    \"\"\"\n    logger = init_logger_file()\n    pfm = ProjFpModel(proj_dir)\n    if not overwrite:\n        for fp in (pfm.cells_df,):\n            if os.path.exists(fp):\n                return logger.warning(file_exists_msg(fp))\n    # Getting region for each detected cell (i.e. row) in cells_df\n    with cluster_process(cls.busy_cluster()):\n        # Reading cells_raw and cells_trfm dataframes\n        cells_df = pd.read_parquet(pfm.cells_raw_df)\n        coords_trfm = pd.read_parquet(pfm.cells_trfm_df)\n        # Sanitising (removing smb columns)\n        cells_df = sanitise_smb_df(cells_df)\n        coords_trfm = sanitise_smb_df(coords_trfm)\n        # Making unique incrementing index\n        cells_df = cells_df.reset_index(drop=True)\n        # Setting the transformed coords\n        cells_df[f\"{Coords.Z.value}_{TRFM}\"] = coords_trfm[Coords.Z.value].values\n        cells_df[f\"{Coords.Y.value}_{TRFM}\"] = coords_trfm[Coords.Y.value].values\n        cells_df[f\"{Coords.X.value}_{TRFM}\"] = coords_trfm[Coords.X.value].values\n\n        # Reading annotation image\n        annot_arr = tifffile.imread(pfm.annot)\n        # Getting the annotation ID for every cell (zyx coord)\n        # Getting transformed coords (that are within tbe bounds_arr, and their corresponding idx)\n        s = annot_arr.shape\n        trfm_loc = (\n            cells_df[\n                [\n                    f\"{Coords.Z.value}_{TRFM}\",\n                    f\"{Coords.Y.value}_{TRFM}\",\n                    f\"{Coords.X.value}_{TRFM}\",\n                ]\n            ]\n            .round(0)\n            .astype(np.int32)\n            .query(\n                f\"({Coords.Z.value}_{TRFM} &gt;= 0) &amp; ({Coords.Z.value}_{TRFM} &lt; {s[0]}) &amp; \"\n                f\"({Coords.Y.value}_{TRFM} &gt;= 0) &amp; ({Coords.Y.value}_{TRFM} &lt; {s[1]}) &amp; \"\n                f\"({Coords.X.value}_{TRFM} &gt;= 0) &amp; ({Coords.X.value}_{TRFM} &lt; {s[2]})\"\n            )\n        )\n        # Getting the pixel values of each valid transformed coord (hence the specified index)\n        # By complex array indexing on ar_annot's (z, y, x) dimensions.\n        # nulls are imputed with -1\n        cells_df[AnnotColumns.ID.value] = pd.Series(\n            annot_arr[*trfm_loc.values.T].astype(np.uint32),\n            index=trfm_loc.index,\n        ).fillna(-1)\n\n        # Reading annotation mappings dataframe\n        annot_df = MapFuncs.annot_dict2df(read_json(pfm.map))\n        # Getting the annotation name for every cell (zyx coord)\n        cells_df = MapFuncs.df_map_ids(cells_df, annot_df)\n        # Saving to disk\n        # NOTE: Using pandas parquet. does not work with dask yet\n        # cells_df = dd.from_pandas(cells_df)\n        write_parquet(cells_df, pfm.cells_df)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#cellcounter.pipeline.pipeline.Pipeline.cellc1","title":"<code>cellc1(proj_dir, overwrite=False, tuning=False)</code>  <code>classmethod</code>","text":"<p>Cell counting pipeline - Step 1</p> <p>Top-hat filter (background subtraction)</p> Source code in <code>cellcounter/pipeline/pipeline.py</code> <pre><code>@classmethod\ndef cellc1(cls, proj_dir: str, overwrite: bool = False, tuning: bool = False) -&gt; None:\n    \"\"\"\n    Cell counting pipeline - Step 1\n\n    Top-hat filter (background subtraction)\n    \"\"\"\n    logger = init_logger_file()\n    pfm = ProjFpModelTuning(proj_dir) if tuning else ProjFpModel(proj_dir)\n    if not overwrite:\n        for fp in (pfm.bgrm,):\n            if os.path.exists(fp):\n                return logger.warning(file_exists_msg(fp))\n    # Making Dask cluster\n    with cluster_process(cls.gpu_cluster()):\n        # Getting configs\n        configs = ConfigParamsModel.read_fp(pfm.config_params)\n        # Reading input images\n        overlap_arr = da.from_zarr(pfm.overlap)\n        # Declaring processing instructions\n        bgrm_arr = da.map_blocks(\n            cls.cellc_funcs.tophat_filt,\n            overlap_arr,\n            configs.tophat_sigma,\n        )\n        # Computing and saving\n        bgrm_arr = disk_cache(bgrm_arr, pfm.bgrm)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#cellcounter.pipeline.pipeline.Pipeline.cellc10","title":"<code>cellc10(proj_dir, overwrite=False, tuning=False)</code>  <code>classmethod</code>","text":"<p>Cell counting pipeline - Step 11</p> <p>Calculate the maxima and watershed, save the cells.</p> <p>Basically a repeat of cellc8 and cellc9 but needs to be done to get the cell volumes in a table. Hence, don't run cellc8 and cellc9 if you don't want to view the cells visually (good for pipeline, not for tuning).</p> Source code in <code>cellcounter/pipeline/pipeline.py</code> <pre><code>@classmethod\ndef cellc10(cls, proj_dir: str, overwrite: bool = False, tuning: bool = False) -&gt; None:\n    \"\"\"\n    Cell counting pipeline - Step 11\n\n    Calculate the maxima and watershed, save the cells.\n\n    Basically a repeat of cellc8 and cellc9 but needs to be done to\n    get the cell volumes in a table. Hence, don't run cellc8 and cellc9 if\n    you don't want to view the cells visually (good for pipeline, not for tuning).\n    \"\"\"\n    logger = init_logger_file()\n    pfm = ProjFpModelTuning(proj_dir) if tuning else ProjFpModel(proj_dir)\n    if not overwrite:\n        for fp in (pfm.cells_raw_df,):\n            if os.path.exists(fp):\n                return logger.warning(file_exists_msg(fp))\n    with cluster_process(cls.heavy_cluster()):\n        # Getting configs\n        configs = ConfigParamsModel.read_fp(pfm.config_params)\n        # Reading input images\n        raw_arr = da.from_zarr(pfm.raw)\n        overlap_arr = da.from_zarr(pfm.overlap)\n        maxima_arr = da.from_zarr(pfm.maxima)\n        threshd_filt_arr = da.from_zarr(pfm.threshd_filt)\n        # Declaring processing instructions\n        # Getting maxima coords and cell measures in table\n        cells_df = block2coords(\n            CpuCellcFuncs.get_cells,\n            raw_arr,\n            overlap_arr,\n            maxima_arr,\n            threshd_filt_arr,\n            configs.overlap_depth,\n        )\n        # Converting from dask to pandas\n        cells_df = cells_df.compute()\n        # Filtering out by volume (same filter cellc9_pipeline volume_filter)\n        cells_df = cells_df.query(\n            f\"({CellColumns.VOLUME.value} &gt;= {configs.min_wshed_size}) &amp; \"\n            f\"({CellColumns.VOLUME.value} &lt;= {configs.max_wshed_size})\"\n        )\n        # Computing and saving as parquet\n        write_parquet(cells_df, pfm.cells_raw_df)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#cellcounter.pipeline.pipeline.Pipeline.cellc10b","title":"<code>cellc10b(proj_dir, overwrite=False, tuning=False)</code>  <code>classmethod</code>","text":"<p>Alternative to cellc10.</p> <p>Uses raw, overlap, maxima_labels, wshed_filt (so wshed computation not run again). Also allows GPU processing.</p> Source code in <code>cellcounter/pipeline/pipeline.py</code> <pre><code>@classmethod\ndef cellc10b(cls, proj_dir: str, overwrite: bool = False, tuning: bool = False) -&gt; None:\n    \"\"\"\n    Alternative to cellc10.\n\n    Uses raw, overlap, maxima_labels, wshed_filt (so wshed computation not run again).\n    Also allows GPU processing.\n    \"\"\"\n    logger = init_logger_file()\n    pfm = ProjFpModelTuning(proj_dir) if tuning else ProjFpModel(proj_dir)\n    if not overwrite:\n        for fp in (pfm.cells_raw_df,):\n            if os.path.exists(fp):\n                return logger.warning(file_exists_msg(fp))\n    with cluster_process(cls.heavy_cluster()):\n        # Getting configs\n        configs = ConfigParamsModel.read_fp(pfm.config_params)\n        # Reading input images\n        raw_arr = da.from_zarr(pfm.raw)\n        overlap_arr = da.from_zarr(pfm.overlap)\n        maxima_labels_arr = da.from_zarr(pfm.maxima_labels)\n        wshed_labels_arr = da.from_zarr(pfm.wshed_labels)\n        wshed_filt_arr = da.from_zarr(pfm.wshed_filt)\n        # Declaring processing instructions\n        # Getting maxima coords and cell measures in table\n        cells_df = block2coords(\n            CpuCellcFuncs.get_cells_b,\n            raw_arr,\n            overlap_arr,\n            maxima_labels_arr,\n            wshed_labels_arr,\n            wshed_filt_arr,\n            configs.overlap_depth,\n        )\n        # Converting from dask to pandas\n        cells_df = cells_df.compute()\n        # Computing and saving as parquet\n        write_parquet(cells_df, pfm.cells_raw_df)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#cellcounter.pipeline.pipeline.Pipeline.cellc2","title":"<code>cellc2(proj_dir, overwrite=False, tuning=False)</code>  <code>classmethod</code>","text":"<p>Cell counting pipeline - Step 2</p> <p>Difference of Gaussians (edge detection)</p> Source code in <code>cellcounter/pipeline/pipeline.py</code> <pre><code>@classmethod\ndef cellc2(cls, proj_dir: str, overwrite: bool = False, tuning: bool = False) -&gt; None:\n    \"\"\"\n    Cell counting pipeline - Step 2\n\n    Difference of Gaussians (edge detection)\n    \"\"\"\n    logger = init_logger_file()\n    pfm = ProjFpModelTuning(proj_dir) if tuning else ProjFpModel(proj_dir)\n    if not overwrite:\n        for fp in (pfm.dog,):\n            if os.path.exists(fp):\n                return logger.warning(file_exists_msg(fp))\n    # Making Dask cluster\n    with cluster_process(cls.gpu_cluster()):\n        # Getting configs\n        configs = ConfigParamsModel.read_fp(pfm.config_params)\n        # Reading input images\n        bgrm_arr = da.from_zarr(pfm.bgrm)\n        # Declaring processing instructions\n        dog_arr = da.map_blocks(\n            cls.cellc_funcs.dog_filt,\n            bgrm_arr,\n            configs.dog_sigma1,\n            configs.dog_sigma2,\n        )\n        # Computing and saving\n        dog_arr = disk_cache(dog_arr, pfm.dog)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#cellcounter.pipeline.pipeline.Pipeline.cellc3","title":"<code>cellc3(proj_dir, overwrite=False, tuning=False)</code>  <code>classmethod</code>","text":"<p>Cell counting pipeline - Step 3</p> <p>Gaussian subtraction with large sigma for adaptive thresholding</p> Source code in <code>cellcounter/pipeline/pipeline.py</code> <pre><code>@classmethod\ndef cellc3(cls, proj_dir: str, overwrite: bool = False, tuning: bool = False) -&gt; None:\n    \"\"\"\n    Cell counting pipeline - Step 3\n\n    Gaussian subtraction with large sigma for adaptive thresholding\n    \"\"\"\n    logger = init_logger_file()\n    pfm = ProjFpModelTuning(proj_dir) if tuning else ProjFpModel(proj_dir)\n    if not overwrite:\n        for fp in (pfm.adaptv,):\n            if os.path.exists(fp):\n                return logger.warning(file_exists_msg(fp))\n    # Making Dask cluster\n    with cluster_process(cls.gpu_cluster()):\n        # Getting configs\n        configs = ConfigParamsModel.read_fp(pfm.config_params)\n        # Reading input images\n        dog_arr = da.from_zarr(pfm.dog)\n        # Declaring processing instructions\n        adaptv_arr = da.map_blocks(\n            cls.cellc_funcs.gauss_subt_filt,\n            dog_arr,\n            configs.large_gauss_sigma,\n        )\n        # Computing and saving\n        adaptv_arr = disk_cache(adaptv_arr, pfm.adaptv)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#cellcounter.pipeline.pipeline.Pipeline.cellc4","title":"<code>cellc4(proj_dir, overwrite=False, tuning=False)</code>  <code>classmethod</code>","text":"<p>Cell counting pipeline - Step 4</p> <p>Currently, manual thresholding. Ideally, mean thresholding with standard deviation offset</p> Source code in <code>cellcounter/pipeline/pipeline.py</code> <pre><code>@classmethod\ndef cellc4(cls, proj_dir: str, overwrite: bool = False, tuning: bool = False) -&gt; None:\n    \"\"\"\n    Cell counting pipeline - Step 4\n\n    Currently, manual thresholding.\n    Ideally, mean thresholding with standard deviation offset\n    \"\"\"\n    logger = init_logger_file()\n    pfm = ProjFpModelTuning(proj_dir) if tuning else ProjFpModel(proj_dir)\n    if not overwrite:\n        for fp in (pfm.threshd,):\n            if os.path.exists(fp):\n                return logger.warning(file_exists_msg(fp))\n    # Making Dask cluster\n    with cluster_process(cls.gpu_cluster()):\n        # Getting configs\n        configs = ConfigParamsModel.read_fp(pfm.config_params)\n        # # Visually inspect sd offset\n        # t_p =adaptv_arr.sum() / (np.prod(adaptv_arr.shape) - (adaptv_arr == 0).sum())\n        # t_p = t_p.compute()\n        # Reading input images\n        adaptv_arr = da.from_zarr(pfm.adaptv)\n        # Declaring processing instructions\n        threshd_arr = da.map_blocks(\n            cls.cellc_funcs.manual_thresh,  # NOTE: previously CPU\n            adaptv_arr,\n            configs.threshd_value,\n        )\n        # Computing and saving\n        threshd_arr = disk_cache(threshd_arr, pfm.threshd)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#cellcounter.pipeline.pipeline.Pipeline.cellc5","title":"<code>cellc5(proj_dir, overwrite=False, tuning=False)</code>  <code>classmethod</code>","text":"<p>Cell counting pipeline - Step 5</p> <p>Getting object sizes</p> Source code in <code>cellcounter/pipeline/pipeline.py</code> <pre><code>@classmethod\ndef cellc5(cls, proj_dir: str, overwrite: bool = False, tuning: bool = False) -&gt; None:\n    \"\"\"\n    Cell counting pipeline - Step 5\n\n    Getting object sizes\n    \"\"\"\n    logger = init_logger_file()\n    pfm = ProjFpModelTuning(proj_dir) if tuning else ProjFpModel(proj_dir)\n    if not overwrite:\n        for fp in (pfm.threshd_volumes,):\n            if os.path.exists(fp):\n                return logger.warning(file_exists_msg(fp))\n    # Making Dask cluster\n    with cluster_process(cls.gpu_cluster()):\n        # Reading input images\n        threshd_arr = da.from_zarr(pfm.threshd)\n        # Declaring processing instructions\n        threshd_volumes_arr = da.map_blocks(\n            cls.cellc_funcs.mask2volume,  # NOTE: previously CPU\n            threshd_arr,\n        )\n        # Computing and saving\n        threshd_volumes_arr = disk_cache(threshd_volumes_arr, pfm.threshd_volumes)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#cellcounter.pipeline.pipeline.Pipeline.cellc6","title":"<code>cellc6(proj_dir, overwrite=False, tuning=False)</code>  <code>classmethod</code>","text":"<p>Cell counting pipeline - Step 6</p> <p>Filter out large objects (likely outlines, not cells)</p> Source code in <code>cellcounter/pipeline/pipeline.py</code> <pre><code>@classmethod\ndef cellc6(cls, proj_dir: str, overwrite: bool = False, tuning: bool = False) -&gt; None:\n    \"\"\"\n    Cell counting pipeline - Step 6\n\n    Filter out large objects (likely outlines, not cells)\n    \"\"\"\n    logger = init_logger_file()\n    pfm = ProjFpModelTuning(proj_dir) if tuning else ProjFpModel(proj_dir)\n    if not overwrite:\n        for fp in (pfm.threshd_filt,):\n            if os.path.exists(fp):\n                return logger.warning(file_exists_msg(fp))\n    with cluster_process(cls.gpu_cluster()):\n        # Getting configs\n        configs = ConfigParamsModel.read_fp(pfm.config_params)\n        # Reading input images\n        threshd_volumes_arr = da.from_zarr(pfm.threshd_volumes)\n        # Declaring processing instructions\n        threshd_filt_arr = da.map_blocks(\n            cls.cellc_funcs.volume_filter,\n            threshd_volumes_arr,\n            configs.min_threshd_size,\n            configs.max_threshd_size,\n        )\n        # Computing and saving\n        threshd_filt_arr = disk_cache(threshd_filt_arr, pfm.threshd_filt)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#cellcounter.pipeline.pipeline.Pipeline.cellc7","title":"<code>cellc7(proj_dir, overwrite=False, tuning=False)</code>  <code>classmethod</code>","text":"<p>Cell counting pipeline - Step 7</p> <p>Get maxima of image masked by labels.</p> Source code in <code>cellcounter/pipeline/pipeline.py</code> <pre><code>@classmethod\ndef cellc7(cls, proj_dir: str, overwrite: bool = False, tuning: bool = False) -&gt; None:\n    \"\"\"\n    Cell counting pipeline - Step 7\n\n    Get maxima of image masked by labels.\n    \"\"\"\n    logger = init_logger_file()\n    pfm = ProjFpModelTuning(proj_dir) if tuning else ProjFpModel(proj_dir)\n    if not overwrite:\n        for fp in (pfm.maxima,):\n            if os.path.exists(fp):\n                return logger.warning(file_exists_msg(fp))\n    with cluster_process(cls.gpu_cluster()):\n        # Getting configs\n        configs = ConfigParamsModel.read_fp(pfm.config_params)\n        # Reading input images\n        overlap_arr = da.from_zarr(pfm.overlap)\n        threshd_filt_arr = da.from_zarr(pfm.threshd_filt)\n        # Declaring processing instructions\n        maxima_arr = da.map_blocks(\n            cls.cellc_funcs.get_local_maxima,\n            overlap_arr,\n            configs.maxima_sigma,\n            threshd_filt_arr,\n        )\n        # Computing and saving\n        maxima_arr = disk_cache(maxima_arr, pfm.maxima)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#cellcounter.pipeline.pipeline.Pipeline.cellc7b","title":"<code>cellc7b(proj_dir, overwrite=False, tuning=False)</code>  <code>classmethod</code>","text":"<p>Cell counting pipeline - Step 7</p> <p>Convert maxima mask to uniquely labelled points.</p> Source code in <code>cellcounter/pipeline/pipeline.py</code> <pre><code>@classmethod\ndef cellc7b(cls, proj_dir: str, overwrite: bool = False, tuning: bool = False) -&gt; None:\n    \"\"\"\n    Cell counting pipeline - Step 7\n\n    Convert maxima mask to uniquely labelled points.\n    \"\"\"\n    # TODO: Check that the results of cellc10 and cellc7b, cellc8a, cellc10a are the same (df)\n    logger = init_logger_file()\n    pfm = ProjFpModelTuning(proj_dir) if tuning else ProjFpModel(proj_dir)\n    if not overwrite:\n        for fp in (pfm.maxima_labels,):\n            if os.path.exists(fp):\n                return logger.warning(file_exists_msg(fp))\n    with cluster_process(cls.gpu_cluster()):\n        # Reading input images\n        maxima_arr = da.from_zarr(pfm.maxima)\n        # Declaring processing instructions\n        maxima_labels_arr = da.map_blocks(\n            cls.cellc_funcs.mask2label,\n            maxima_arr,\n        )\n        maxima_labels_arr = disk_cache(maxima_labels_arr, pfm.maxima_labels)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#cellcounter.pipeline.pipeline.Pipeline.cellc8","title":"<code>cellc8(proj_dir, overwrite=False, tuning=False)</code>  <code>classmethod</code>","text":"<p>Cell counting pipeline - Step 8</p> <p>Watershed segmentation volumes.</p> Source code in <code>cellcounter/pipeline/pipeline.py</code> <pre><code>@classmethod\ndef cellc8(cls, proj_dir: str, overwrite: bool = False, tuning: bool = False) -&gt; None:\n    \"\"\"\n    Cell counting pipeline - Step 8\n\n    Watershed segmentation volumes.\n    \"\"\"\n    logger = init_logger_file()\n    pfm = ProjFpModelTuning(proj_dir) if tuning else ProjFpModel(proj_dir)\n    if not overwrite:\n        for fp in (pfm.wshed_volumes,):\n            if os.path.exists(fp):\n                return logger.warning(file_exists_msg(fp))\n    with cluster_process(cls.heavy_cluster()):\n        # n_workers=2\n        # Reading input images\n        overlap_arr = da.from_zarr(pfm.overlap)\n        maxima_arr = da.from_zarr(pfm.maxima)\n        threshd_filt_arr = da.from_zarr(pfm.threshd_filt)\n        # Declaring processing instructions\n        wshed_volumes_arr = da.map_blocks(\n            CpuCellcFuncs.wshed_segm_volumes,\n            overlap_arr,\n            maxima_arr,\n            threshd_filt_arr,\n        )\n        wshed_volumes_arr = disk_cache(wshed_volumes_arr, pfm.wshed_volumes)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#cellcounter.pipeline.pipeline.Pipeline.cellc9","title":"<code>cellc9(proj_dir, overwrite=False, tuning=False)</code>  <code>classmethod</code>","text":"<p>Cell counting pipeline - Step 9</p> <p>Filter out large watershed objects (again cell areas, not cells).</p> Source code in <code>cellcounter/pipeline/pipeline.py</code> <pre><code>@classmethod\ndef cellc9(cls, proj_dir: str, overwrite: bool = False, tuning: bool = False) -&gt; None:\n    \"\"\"\n    Cell counting pipeline - Step 9\n\n    Filter out large watershed objects (again cell areas, not cells).\n    \"\"\"\n    logger = init_logger_file()\n    pfm = ProjFpModelTuning(proj_dir) if tuning else ProjFpModel(proj_dir)\n    if not overwrite:\n        for fp in (pfm.wshed_filt,):\n            if os.path.exists(fp):\n                return logger.warning(file_exists_msg(fp))\n    with cluster_process(cls.gpu_cluster()):\n        # Getting configs\n        configs = ConfigParamsModel.read_fp(pfm.config_params)\n        # Reading input images\n        wshed_volumes_arr = da.from_zarr(pfm.wshed_volumes)\n        # Declaring processing instructions\n        wshed_filt_arr = da.map_blocks(\n            cls.cellc_funcs.volume_filter,\n            wshed_volumes_arr,\n            configs.min_wshed_size,\n            configs.max_wshed_size,\n        )\n        # Computing and saving\n        wshed_filt_arr = disk_cache(wshed_filt_arr, pfm.wshed_filt)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#cellcounter.pipeline.pipeline.Pipeline.group_cells","title":"<code>group_cells(proj_dir, overwrite=False)</code>  <code>classmethod</code>","text":"<p>Grouping cells by region name and aggregating total cell volume and cell count for each region.</p> <p>NOTE: saves the cells_agg dataframe as pandas parquet.</p> Source code in <code>cellcounter/pipeline/pipeline.py</code> <pre><code>@classmethod\ndef group_cells(cls, proj_dir: str, overwrite: bool = False) -&gt; None:\n    \"\"\"\n    Grouping cells by region name and aggregating total cell volume\n    and cell count for each region.\n\n    NOTE: saves the cells_agg dataframe as pandas parquet.\n    \"\"\"\n    logger = init_logger_file()\n    pfm = ProjFpModel(proj_dir)\n    if not overwrite:\n        for fp in (pfm.cells_agg_df,):\n            if os.path.exists(fp):\n                return logger.warning(file_exists_msg(fp))\n    # Making cells_agg_df\n    with cluster_process(cls.busy_cluster()):\n        # Reading cells dataframe\n        cells_df = pd.read_parquet(pfm.cells_df)\n        # Sanitising (removing smb columns)\n        cells_df = sanitise_smb_df(cells_df)\n        # Grouping cells by region name and aggregating on given mappings\n        cells_agg_df = cells_df.groupby(AnnotColumns.ID.value).agg(CELL_AGG_MAPPINGS)\n        cells_agg_df.columns = list(CELL_AGG_MAPPINGS.keys())\n        # Reading annotation mappings dataframe\n        # Making df of region names and their parent region names\n        annot_df = MapFuncs.annot_dict2df(read_json(pfm.map))\n        # Combining (summing) the cells_agg_df values for parent regions using the annot_df\n        cells_agg_df = MapFuncs.combine_nested_regions(cells_agg_df, annot_df)\n        # Calculating integrated average intensity (sum_intensity / volume)\n        cells_agg_df[CellColumns.IOV.value] = (\n            cells_agg_df[CellColumns.SUM_INTENSITY.value] / cells_agg_df[CellColumns.VOLUME.value]\n        )\n        # Selecting and ordering relevant columns\n        cells_agg_df = cells_agg_df[[*ANNOT_COLUMNS_FINAL, *enum2list(CellColumns)]]\n        # Saving to disk\n        # NOTE: Using pandas parquet. does not work with dask yet\n        # cells_agg = dd.from_pandas(cells_agg)\n        write_parquet(cells_agg_df, pfm.cells_agg_df)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#cellcounter.pipeline.pipeline.Pipeline.make_mask","title":"<code>make_mask(proj_dir, overwrite=False)</code>  <code>classmethod</code>","text":"<p>Makes mask of actual image in reference space. Also stores # and proportion of existent voxels for each region.</p> Source code in <code>cellcounter/pipeline/pipeline.py</code> <pre><code>@classmethod\ndef make_mask(cls, proj_dir: str, overwrite: bool = False) -&gt; None:\n    \"\"\"\n    Makes mask of actual image in reference space.\n    Also stores # and proportion of existent voxels\n    for each region.\n    \"\"\"\n    logger = init_logger_file()\n    pfm = ProjFpModel(proj_dir)\n    if not overwrite:\n        for fp in (pfm.mask_fill, pfm.mask_outline, pfm.mask_reg, pfm.mask_df):\n            if os.path.exists(fp):\n                return logger.warning(file_exists_msg(fp))\n    # Getting configs\n    configs = ConfigParamsModel.read_fp(pfm.config_params)\n    # Reading annot img (proj oriented and trimmed) and bounded img\n    annot_arr = tifffile.imread(pfm.annot)\n    bounded_arr = tifffile.imread(pfm.bounded)\n    # Storing annot_arr shape\n    s = annot_arr.shape\n    # Making mask\n    blur_arr = cls.cellc_funcs.gauss_blur_filt(bounded_arr, configs.mask_gaus_blur)\n    write_tiff(blur_arr, pfm.premask_blur)\n    mask_arr = cls.cellc_funcs.manual_thresh(blur_arr, configs.mask_thresh)\n    write_tiff(mask_arr, pfm.mask_fill)\n\n    # Make outline\n    outline_df = MaskFuncs.make_outline(mask_arr)\n    # Transformix on coords\n    outline_df[[Coords.Z.value, Coords.Y.value, Coords.X.value]] = (\n        ElastixFuncs.transformation_coords(\n            outline_df,\n            pfm.ref,\n            pfm.regresult,\n        )[[Coords.Z.value, Coords.Y.value, Coords.X.value]]\n        .round(0)\n        .astype(np.int32)\n    )\n    # Filtering out of bounds coords\n    outline_df = outline_df.query(\n        f\"({Coords.Z.value} &gt;= 0) &amp; ({Coords.Z.value} &lt; {s[0]}) &amp; \"\n        f\"({Coords.Y.value} &gt;= 0) &amp; ({Coords.Y.value} &lt; {s[1]}) &amp; \"\n        f\"({Coords.X.value} &gt;= 0) &amp; ({Coords.X.value} &lt; {s[2]})\"\n    )\n\n    # Make outline img (1 for in, 2 for out)\n    # TODO: convert to return np.array and save out-of-function\n    VisualCheckFuncsTiff.coords2points(outline_df[outline_df.is_in == 1], s, pfm.mask_outline)\n    in_arr = tifffile.imread(pfm.mask_outline)\n    VisualCheckFuncsTiff.coords2points(outline_df[outline_df.is_in == 0], s, pfm.mask_outline)\n    out_arr = tifffile.imread(pfm.mask_outline)\n    write_tiff(in_arr + out_arr * 2, pfm.mask_outline)\n\n    # Fill in outline to recreate mask (not perfect)\n    mask_reg_arr = MaskFuncs.fill_outline(outline_df, s)\n    # Opening (removes FP) and closing (fills FN)\n    mask_reg_arr = ndimage.binary_closing(mask_reg_arr, iterations=2).astype(np.uint8)\n    mask_reg_arr = ndimage.binary_opening(mask_reg_arr, iterations=2).astype(np.uint8)\n    # Saving\n    write_tiff(mask_reg_arr, pfm.mask_reg)\n\n    # Counting mask voxels in each region\n    # Getting original annot fp by making ref_fp_model\n    rfm = RefFpModel(\n        configs.atlas_dir,\n        configs.ref_version,\n        configs.annot_version,\n        configs.map_version,\n    )\n    # Reading original annot\n    annot_orig_arr = tifffile.imread(rfm.annot)\n    # Getting the annotation name for every cell (zyx coord)\n    mask_df = pd.merge(\n        left=MaskFuncs.mask2region_counts(np.full(annot_orig_arr.shape, 1), annot_orig_arr),\n        right=MaskFuncs.mask2region_counts(mask_reg_arr, annot_arr),\n        how=\"left\",\n        left_index=True,\n        right_index=True,\n        suffixes=(\"_annot\", \"_mask\"),\n    ).fillna(0)\n    # Reading annotation mappings json\n    annot_df = MapFuncs.annot_dict2df(read_json(pfm.map))\n    # Combining (summing) the mask_df volumes for parent regions using the annot_df\n    mask_df = MapFuncs.combine_nested_regions(mask_df, annot_df)\n    # Calculating proportion of mask volume in each region\n    mask_df[MaskColumns.VOLUME_PROP.value] = (\n        mask_df[MaskColumns.VOLUME_MASK.value] / mask_df[MaskColumns.VOLUME_ANNOT.value]\n    )\n    # Selecting and ordering relevant columns\n    mask_df = mask_df[[*ANNOT_COLUMNS_FINAL, *enum2list(MaskColumns)]]\n    # Saving\n    write_parquet(mask_df, pfm.mask_df)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#cellcounter.pipeline.pipeline.Pipeline.make_tuning_arr","title":"<code>make_tuning_arr(proj_dir, overwrite=False)</code>  <code>classmethod</code>","text":"<p>Crop raw zarr to make a smaller zarr for tuning the cell counting pipeline.</p> Source code in <code>cellcounter/pipeline/pipeline.py</code> <pre><code>@classmethod\ndef make_tuning_arr(cls, proj_dir: str, overwrite: bool = False) -&gt; None:\n    \"\"\"\n    Crop raw zarr to make a smaller zarr for tuning the cell counting pipeline.\n    \"\"\"\n    logger = init_logger_file()\n    logger.debug(\"Converting/ensuring pfm is production filepaths (copy)\")\n    pfm = ProjFpModel(proj_dir)\n    pfm_tuning = ProjFpModelTuning(proj_dir)\n    logger.debug(\"Reading config params\")\n    configs = ConfigParamsModel.read_fp(pfm.config_params)\n    with cluster_process(cls.busy_cluster()):  # TODO:  is this faster without cluster?\n        logger.debug(\"Reading raw zarr\")\n        raw_arr = da.from_zarr(pfm.raw)\n        logger.debug(\"Cropping raw zarr\")\n        raw_arr = raw_arr[\n            slice(*configs.tuning_z_trim),\n            slice(*configs.tuning_y_trim),\n            slice(*configs.tuning_x_trim),\n        ]\n        if not overwrite:\n            for fp in (pfm_tuning.raw,):\n                if os.path.exists(fp):\n                    return logger.warning(file_exists_msg(fp))\n        logger.debug(\"Saving cropped raw zarr\")\n        raw_arr = disk_cache(raw_arr, pfm_tuning.raw)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#cellcounter.pipeline.pipeline.Pipeline.run_pipeline","title":"<code>run_pipeline(in_fp, proj_dir, overwrite=False, **kwargs)</code>  <code>classmethod</code>","text":"<p>Running all pipelines in order.</p> Source code in <code>cellcounter/pipeline/pipeline.py</code> <pre><code>@classmethod\ndef run_pipeline(cls, in_fp: str, proj_dir: str, overwrite: bool = False, **kwargs) -&gt; None:\n    \"\"\"\n    Running all pipelines in order.\n    \"\"\"\n    # Updating project configs\n    cls.update_configs(proj_dir, **kwargs)\n    # tiff to zarr\n    cls.tiff2zarr(proj_dir, in_fp, overwrite=overwrite)\n    # Registration\n    cls.reg_ref_prepare(proj_dir, overwrite=overwrite)\n    cls.reg_img_rough(proj_dir, overwrite=overwrite)\n    cls.reg_img_fine(proj_dir, overwrite=overwrite)\n    cls.reg_img_trim(proj_dir, overwrite=overwrite)\n    cls.reg_img_bound(proj_dir, overwrite=overwrite)\n    cls.reg_elastix(proj_dir, overwrite=overwrite)\n    # Coverage mask\n    cls.make_mask(proj_dir, overwrite=overwrite)\n    # Making trimmed image for cell count tuning\n    cls.make_tuning_arr(proj_dir, overwrite=overwrite)\n    # Cell counting (tuning and final)\n    for is_tuning in [True, False]:\n        cls.img_overlap(proj_dir, overwrite=overwrite, tuning=is_tuning)\n        cls.cellc1(proj_dir, overwrite=overwrite, tuning=is_tuning)\n        cls.cellc2(proj_dir, overwrite=overwrite, tuning=is_tuning)\n        cls.cellc3(proj_dir, overwrite=overwrite, tuning=is_tuning)\n        cls.cellc4(proj_dir, overwrite=overwrite, tuning=is_tuning)\n        cls.cellc5(proj_dir, overwrite=overwrite, tuning=is_tuning)\n        cls.cellc6(proj_dir, overwrite=overwrite, tuning=is_tuning)\n        cls.cellc7(proj_dir, overwrite=overwrite, tuning=is_tuning)\n        cls.cellc8(proj_dir, overwrite=overwrite, tuning=is_tuning)\n        cls.cellc9(proj_dir, overwrite=overwrite, tuning=is_tuning)\n        cls.cellc10(proj_dir, overwrite=overwrite, tuning=is_tuning)\n    # Cell mapping\n    cls.transform_coords(proj_dir, overwrite=overwrite)\n    cls.cell_mapping(proj_dir, overwrite=overwrite)\n    cls.group_cells(proj_dir, overwrite=overwrite)\n    cls.cells2csv(proj_dir, overwrite=overwrite)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#cellcounter.pipeline.pipeline.Pipeline.tiff2zarr","title":"<code>tiff2zarr(proj_dir, in_fp, overwrite=False)</code>  <code>classmethod</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>pfm</code> <code>ProjFpModel</code> <p>description</p> required <code>in_fp</code> <code>str</code> <p>description</p> required <code>overwrite</code> <code>bool</code> <p>description, by default False</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>description</p> Source code in <code>cellcounter/pipeline/pipeline.py</code> <pre><code>@classmethod\ndef tiff2zarr(cls, proj_dir: str, in_fp: str, overwrite: bool = False) -&gt; None:\n    \"\"\"\n    _summary_\n\n    Parameters\n    ----------\n    pfm : ProjFpModel\n        _description_\n    in_fp : str\n        _description_\n    overwrite : bool, optional\n        _description_, by default False\n\n    Raises\n    ------\n    ValueError\n        _description_\n    \"\"\"\n    logger = init_logger_file()\n    pfm = ProjFpModel(proj_dir)\n    if not overwrite:\n        for fp in (pfm.raw,):\n            if os.path.exists(fp):\n                return logger.warning(file_exists_msg(fp))\n    logger.debug(\"Reading config params\")\n    configs = ConfigParamsModel.read_fp(pfm.config_params)\n    logger.debug(\"Making zarr from tiff file(s)\")\n    with cluster_process(LocalCluster(n_workers=1, threads_per_worker=6)):  # TODO: is this faster without cluster?\n        if os.path.isdir(in_fp):\n            logger.debug(f\"in_fp ({in_fp}) is a directory\")\n            logger.debug(\"Making zarr from tiff file stack in directory\")\n            Tiff2ZarrFuncs.tiffs2zarr(\n                in_fp_ls=tuple(\n                    natsorted((os.path.join(in_fp, i) for i in os.listdir(in_fp) if re.search(r\".tif$\", i)))\n                ),\n                out_fp=pfm.raw,\n                chunks=configs.zarr_chunksize,\n            )\n        elif os.path.isfile(in_fp):\n            logger.debug(f\"in_fp ({in_fp}) is a file\")\n            logger.debug(\"Making zarr from big-tiff file\")\n            Tiff2ZarrFuncs.btiff2zarr(\n                in_fp=in_fp,\n                out_fp=pfm.raw,\n                chunks=configs.zarr_chunksize,\n            )\n        else:\n            raise ValueError(f'Input file path, \"{in_fp}\" does not exist.')\n</code></pre>"},{"location":"reference/pipeline_funcs.html#cellcounter.pipeline.pipeline.Pipeline.transform_coords","title":"<code>transform_coords(proj_dir, overwrite=False)</code>  <code>classmethod</code>","text":"<p><code>in_id</code> and <code>out_id</code> are either maxima or region</p> <p>NOTE: saves the cells_trfm dataframe as pandas parquet.</p> Source code in <code>cellcounter/pipeline/pipeline.py</code> <pre><code>@classmethod\ndef transform_coords(cls, proj_dir: str, overwrite: bool = False) -&gt; None:\n    \"\"\"\n    `in_id` and `out_id` are either maxima or region\n\n    NOTE: saves the cells_trfm dataframe as pandas parquet.\n    \"\"\"\n    logger = init_logger_file()\n    pfm = ProjFpModel(proj_dir)\n    if not overwrite:\n        for fp in (pfm.cells_trfm_df,):\n            if os.path.exists(fp):\n                return logger.warning(file_exists_msg(fp))\n    # Getting configs\n    configs = ConfigParamsModel.read_fp(pfm.config_params)\n    with cluster_process(cls.busy_cluster()):\n        # Setting output key (in the form \"&lt;maxima/region&gt;_trfm_df\")\n        # Getting cell coords\n        cells_df = pd.read_parquet(pfm.cells_raw_df)\n        # Sanitising (removing smb columns)\n        cells_df = sanitise_smb_df(cells_df)\n        # Taking only Coords.Z.value, Coords.Y.value, Coords.X.value coord columns\n        cells_df = cells_df[enum2list(Coords)]\n        # Scaling to resampled rough space\n        # NOTE: this downsampling uses slicing so must be computed differently\n        cells_df = cells_df / np.array((configs.z_rough, configs.y_rough, configs.x_rough))\n        # Scaling to resampled space\n        cells_df = cells_df * np.array((configs.z_fine, configs.y_fine, configs.x_fine))\n        # Trimming/offsetting to sliced space\n        cells_df = cells_df - np.array([s[0] or 0 for s in (configs.z_trim, configs.y_trim, configs.x_trim)])\n        # Converting back to DataFrame\n        cells_df = pd.DataFrame(cells_df, columns=enum2list(Coords))\n\n        cells_trfm_df = ElastixFuncs.transformation_coords(cells_df, pfm.ref, pfm.regresult)\n        # NOTE: Using pandas parquet. does not work with dask yet\n        # cells_df = dd.from_pandas(cells_df, npartitions=1)\n        # Fitting resampled space to atlas image with Transformix (from Elastix registration step)\n        # cells_df = cells_df.repartition(\n        #     npartitions=int(np.ceil(cells_df.shape[0].compute() / ROWSPPART))\n        # )\n        # cells_df = cells_df.map_partitions(\n        #     ElastixFuncs.transformation_coords, pfm.ref.val, pfm.regresult.val\n        # )\n        write_parquet(cells_trfm_df, pfm.cells_trfm_df)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#cellcounter.pipeline.pipeline.Pipeline.update_configs","title":"<code>update_configs(proj_dir, **kwargs)</code>  <code>classmethod</code>","text":"<p>If config_params file does not exist, makes a new one.</p> <p>Then updates the config_params file with the kwargs. If there are no kwargs, will not update the file (other than making it if it did not exist).</p> <p>Also creates all the project sub-directories too.</p> <p>Finally, returns the ConfigParamsModel object.</p> Source code in <code>cellcounter/pipeline/pipeline.py</code> <pre><code>@classmethod\ndef update_configs(cls, proj_dir: str, **kwargs) -&gt; ConfigParamsModel:\n    \"\"\"\n    If config_params file does not exist, makes a new one.\n\n    Then updates the config_params file with the kwargs.\n    If there are no kwargs, will not update the file\n    (other than making it if it did not exist).\n\n    Also creates all the project sub-directories too.\n\n    Finally, returns the ConfigParamsModel object.\n    \"\"\"\n    logger = init_logger_file()\n    pfm = ProjFpModel(proj_dir)\n    logger.debug(\"Making all the project sub-directories\")\n    logger.debug(\"Reading/creating params json\")\n    try:\n        configs = ConfigParamsModel.read_fp(pfm.config_params)\n        logger.debug(\"The configs file exists so using this file.\")\n    except FileNotFoundError:\n        logger.debug(\"The configs file does NOT exists.\")\n        configs = ConfigParamsModel()\n        logger.debug(\"Saving newly created configs file.\")\n        write_json(pfm.config_params, configs.model_dump())\n    if kwargs != {}:\n        logger.debug(f\"kwargs is not empty. They are: {kwargs}\")\n        configs_new = configs.model_validate(configs.model_copy(update=kwargs))\n        if configs_new != configs:\n            logger.debug(\"New configs are different from old configs. Overwriting to file.\")\n            write_json(pfm.config_params, configs_new.model_dump())\n    logger.debug(\"Returning the configs file\")\n    return configs\n</code></pre>"},{"location":"reference/viewer_funcs.html","title":"Viewer funcs","text":""},{"location":"reference/viewer_funcs.html#cellcounter.pipeline.visual_check.VisualCheck","title":"<code>cellcounter.pipeline.visual_check.VisualCheck</code>","text":"Source code in <code>cellcounter/pipeline/visual_check.py</code> <pre><code>class VisualCheck:\n    # Clusters\n    # busy (many workers - carrying low RAM computations)\n    n_workers = 6\n    threads_per_worker = 2\n\n    @classmethod\n    def cluster(cls):\n        return LocalCluster(n_workers=cls.n_workers, threads_per_worker=cls.threads_per_worker)\n\n    ###################################################################################################\n    # VISUAL CHECKS FROM DF POINTS\n    ###################################################################################################\n\n    @classmethod\n    def cellc_trim_to_final(cls, proj_dir: str, overwrite: bool = False, tuning: bool = False) -&gt; None:\n        \"\"\"\n        Cell counting pipeline - Step 10\n\n        Trimming filtered regions overlaps to make:\n        - Trimmed maxima image\n        - Trimmed threshold image\n        - Trimmed watershed image\n        \"\"\"\n        logger = init_logger_file()\n        pfm = ProjFpModelTuning(proj_dir) if tuning else ProjFpModel(proj_dir)\n        if not overwrite:\n            for fp in (pfm.maxima_final, pfm.threshd_final, pfm.wshed_final):\n                if os.path.exists(fp):\n                    return logger.warning(file_exists_msg(fp))\n        with cluster_process(cls.cluster()):\n            # Getting configs\n            configs = ConfigParamsModel.read_fp(pfm.config_params)\n            # Reading input images\n            maxima_arr = da.from_zarr(pfm.maxima)\n            threshd_filt_arr = da.from_zarr(pfm.threshd_filt)\n            wshed_volumes_arr = da.from_zarr(pfm.wshed_volumes)\n            # Declaring processing instructions\n            maxima_final_arr = da_trim(maxima_arr, d=configs.overlap_depth)\n            threshd_final_arr = da_trim(threshd_filt_arr, d=configs.overlap_depth)\n            wshed_final_arr = da_trim(wshed_volumes_arr, d=configs.overlap_depth)\n            # Computing and saving\n            maxima_final_arr = disk_cache(maxima_final_arr, pfm.maxima_final)\n            threshd_final_arr = disk_cache(threshd_final_arr, pfm.threshd_final)\n            wshed_final_arr = disk_cache(wshed_final_arr, pfm.wshed_final)\n\n    @classmethod\n    def coords2points_raw(cls, proj_dir: str, overwrite: bool = False, tuning: bool = False) -&gt; None:\n        logger = init_logger_file()\n        pfm = ProjFpModelTuning(proj_dir) if tuning else ProjFpModel(proj_dir)\n        if not overwrite:\n            for fp in (pfm.points_raw,):\n                if os.path.exists(fp):\n                    return logger.warning(file_exists_msg(fp))\n        with cluster_process(cls.cluster()):\n            VisualCheckFuncsDask.coords2points(\n                coords=pd.read_parquet(pfm.cells_raw_df),\n                shape=da.from_zarr(pfm.raw).shape,\n                out_fp=pfm.points_raw,\n            )\n\n    @classmethod\n    def coords2heatmap_raw(cls, proj_dir: str, overwrite: bool = False, tuning: bool = False) -&gt; None:\n        logger = init_logger_file()\n        pfm = ProjFpModelTuning(proj_dir) if tuning else ProjFpModel(proj_dir)\n        if not overwrite:\n            for fp in (pfm.heatmap_raw,):\n                if os.path.exists(fp):\n                    return logger.warning(file_exists_msg(fp))\n        with cluster_process(cls.cluster()):\n            configs = ConfigParamsModel.read_fp(pfm.config_params)\n            VisualCheckFuncsDask.coords2heatmap(\n                coords=pd.read_parquet(pfm.cells_raw_df),\n                shape=da.from_zarr(pfm.raw).shape,\n                out_fp=pfm.heatmap_raw,\n                radius=configs.heatmap_raw_radius,\n            )\n\n    @classmethod\n    def coords2points_trfm(cls, proj_dir: str, overwrite: bool = False) -&gt; None:\n        logger = init_logger_file()\n        pfm = ProjFpModel(proj_dir)\n        if not overwrite:\n            for fp in (pfm.points_trfm,):\n                if os.path.exists(fp):\n                    return logger.warning(file_exists_msg(fp))\n        VisualCheckFuncsTiff.coords2points(\n            coords=pd.read_parquet(pfm.cells_trfm_df),\n            shape=tifffile.imread(pfm.ref).shape,\n            out_fp=pfm.points_trfm,\n        )\n\n    @classmethod\n    def coords2heatmap_trfm(cls, proj_dir: str, overwrite: bool = False) -&gt; None:\n        logger = init_logger_file()\n        pfm = ProjFpModel(proj_dir)\n        if not overwrite:\n            for fp in (pfm.heatmap_trfm,):\n                if os.path.exists(fp):\n                    return logger.warning(file_exists_msg(fp))\n        configs = ConfigParamsModel.read_fp(pfm.config_params)\n        VisualCheckFuncsTiff.coords2heatmap(\n            coords=pd.read_parquet(pfm.cells_trfm_df),\n            shape=tifffile.imread(pfm.ref).shape,\n            out_fp=pfm.heatmap_trfm,\n            radius=configs.heatmap_trfm_radius,\n        )\n\n    ###################################################################################################\n    # COMBINING/MERGING ARRAYS IN RGB LAYERS\n    ###################################################################################################\n\n    @classmethod\n    def combine_reg(cls, proj_dir: str, overwrite: bool = False) -&gt; None:\n        logger = init_logger_file()\n        pfm = ProjFpModel(proj_dir)\n        if not overwrite:\n            for fp in (pfm.comb_reg,):\n                if os.path.exists(fp):\n                    return logger.warning(file_exists_msg(fp))\n        ViewerFuncs.combine_arrs(\n            fp_in_ls=(pfm.trimmed, pfm.bounded, pfm.regresult),\n            fp_out=pfm.comb_reg,\n        )\n\n    @classmethod\n    def combine_cellc(cls, proj_dir: str, overwrite: bool = False, tuning: bool = False) -&gt; None:\n        logger = init_logger_file()\n        pfm = ProjFpModelTuning(proj_dir) if tuning else ProjFpModel(proj_dir)\n        if not overwrite:\n            for fp in (pfm.comb_cellc,):\n                if os.path.exists(fp):\n                    return logger.warning(file_exists_msg(fp))\n        configs = ConfigParamsModel.read_fp(pfm.config_params)\n        z_trim = slice(None)\n        y_trim = slice(None)\n        x_trim = slice(None)\n        if not tuning:\n            z_trim = slice(*configs.combine_cellc_z_trim)\n            y_trim = slice(*configs.combine_cellc_y_trim)\n            x_trim = slice(*configs.combine_cellc_x_trim)\n        ViewerFuncs.combine_arrs(\n            fp_in_ls=(pfm.raw, pfm.threshd_final, pfm.wshed_final),\n            fp_out=pfm.comb_cellc,\n            trimmer=(z_trim, y_trim, x_trim),\n        )\n\n    @classmethod\n    def combine_heatmap_trfm(cls, proj_dir: str, overwrite: bool = False) -&gt; None:\n        logger = init_logger_file()\n        pfm = ProjFpModel(proj_dir)\n        if not overwrite:\n            for fp in (pfm.comb_heatmap,):\n                if os.path.exists(fp):\n                    return logger.warning(file_exists_msg(fp))\n        ViewerFuncs.combine_arrs(\n            fp_in_ls=(pfm.ref, pfm.annot, pfm.heatmap_trfm),\n            # 2nd regresult means the combining works in ImageJ\n            fp_out=pfm.comb_heatmap,\n        )\n\n    ###################################################################################################\n    # ALL PIPELINE FUNCTION\n    ###################################################################################################\n\n    @classmethod\n    def run_make_visual_checks(cls, proj_dir: str, overwrite: bool = False) -&gt; None:\n        \"\"\"\n        Running all visual check pipelines in order.\n        \"\"\"\n        # Cell counting visual checks\n        for is_tuning in [True, False]:\n            cls.cellc_trim_to_final(proj_dir, overwrite=overwrite, tuning=is_tuning)\n            cls.coords2points_raw(proj_dir, overwrite=overwrite, tuning=is_tuning)\n            cls.combine_cellc(proj_dir, overwrite=overwrite, tuning=is_tuning)\n        # Registration visual check\n        cls.combine_reg(proj_dir, overwrite=overwrite)\n        # Transformed space visual checks\n        cls.coords2points_trfm(proj_dir, overwrite=overwrite)\n        cls.coords2heatmap_trfm(proj_dir, overwrite=overwrite)\n        cls.combine_heatmap_trfm(proj_dir, overwrite=overwrite)\n</code></pre>"},{"location":"reference/viewer_funcs.html#cellcounter.pipeline.visual_check.VisualCheck.cellc_trim_to_final","title":"<code>cellc_trim_to_final(proj_dir, overwrite=False, tuning=False)</code>  <code>classmethod</code>","text":"<p>Cell counting pipeline - Step 10</p> <p>Trimming filtered regions overlaps to make: - Trimmed maxima image - Trimmed threshold image - Trimmed watershed image</p> Source code in <code>cellcounter/pipeline/visual_check.py</code> <pre><code>@classmethod\ndef cellc_trim_to_final(cls, proj_dir: str, overwrite: bool = False, tuning: bool = False) -&gt; None:\n    \"\"\"\n    Cell counting pipeline - Step 10\n\n    Trimming filtered regions overlaps to make:\n    - Trimmed maxima image\n    - Trimmed threshold image\n    - Trimmed watershed image\n    \"\"\"\n    logger = init_logger_file()\n    pfm = ProjFpModelTuning(proj_dir) if tuning else ProjFpModel(proj_dir)\n    if not overwrite:\n        for fp in (pfm.maxima_final, pfm.threshd_final, pfm.wshed_final):\n            if os.path.exists(fp):\n                return logger.warning(file_exists_msg(fp))\n    with cluster_process(cls.cluster()):\n        # Getting configs\n        configs = ConfigParamsModel.read_fp(pfm.config_params)\n        # Reading input images\n        maxima_arr = da.from_zarr(pfm.maxima)\n        threshd_filt_arr = da.from_zarr(pfm.threshd_filt)\n        wshed_volumes_arr = da.from_zarr(pfm.wshed_volumes)\n        # Declaring processing instructions\n        maxima_final_arr = da_trim(maxima_arr, d=configs.overlap_depth)\n        threshd_final_arr = da_trim(threshd_filt_arr, d=configs.overlap_depth)\n        wshed_final_arr = da_trim(wshed_volumes_arr, d=configs.overlap_depth)\n        # Computing and saving\n        maxima_final_arr = disk_cache(maxima_final_arr, pfm.maxima_final)\n        threshd_final_arr = disk_cache(threshd_final_arr, pfm.threshd_final)\n        wshed_final_arr = disk_cache(wshed_final_arr, pfm.wshed_final)\n</code></pre>"},{"location":"reference/viewer_funcs.html#cellcounter.pipeline.visual_check.VisualCheck.run_make_visual_checks","title":"<code>run_make_visual_checks(proj_dir, overwrite=False)</code>  <code>classmethod</code>","text":"<p>Running all visual check pipelines in order.</p> Source code in <code>cellcounter/pipeline/visual_check.py</code> <pre><code>@classmethod\ndef run_make_visual_checks(cls, proj_dir: str, overwrite: bool = False) -&gt; None:\n    \"\"\"\n    Running all visual check pipelines in order.\n    \"\"\"\n    # Cell counting visual checks\n    for is_tuning in [True, False]:\n        cls.cellc_trim_to_final(proj_dir, overwrite=overwrite, tuning=is_tuning)\n        cls.coords2points_raw(proj_dir, overwrite=overwrite, tuning=is_tuning)\n        cls.combine_cellc(proj_dir, overwrite=overwrite, tuning=is_tuning)\n    # Registration visual check\n    cls.combine_reg(proj_dir, overwrite=overwrite)\n    # Transformed space visual checks\n    cls.coords2points_trfm(proj_dir, overwrite=overwrite)\n    cls.coords2heatmap_trfm(proj_dir, overwrite=overwrite)\n    cls.combine_heatmap_trfm(proj_dir, overwrite=overwrite)\n</code></pre>"},{"location":"tutorials/pipeline.html","title":"Pipeline","text":""},{"location":"tutorials/tutorial.html","title":"Setup","text":"<p>Before running the behavysis_pipeline analysises, the files that we want to analyse must be set up a certain way for the behavysis_pipeline program to recognise them.</p> <p>There are three important guidelines to set up the project:</p> <ul> <li>Structure of files in folders .</li> <li>Experiment files.</li> <li>Config files for each experiment.</li> </ul>"},{"location":"tutorials/tutorial.html#folder-structure","title":"Folder Structure","text":"<p>They need to be set up inside specially named folders, as shown below.</p> <p>An example of how this would look on a computer (in this case, a Mac) is shown below.</p>"},{"location":"tutorials/tutorial.html#experiment-files","title":"Experiment Files","text":"<p>Each experiment must have files that have same name (not including the suffix like <code>.csv</code> or <code>.mp4</code>). An example is \"day1_experiment1\" must have all files named \"day1_experiment1.mp4\", \"day1_experiment1.csv\", \"day1_experiment1.json\" etc. stored in the corresponding folder.</p>"},{"location":"tutorials/tutorial.html#config-files","title":"Config Files","text":"<p>The config file for an experiment stores all the parameters for how the experiment was recorded (e.g., the frames per second of the raw video, the experiment duration, etc.), and the parameters for how we want to process the data (e.g., the intended frames per second to format the video to, the DLC model to use to analyse, the likeliness pcutoff to interpolate points, etc.)</p> <p>An example of a config file is shown here.</p>"},{"location":"tutorials/tutorial.html#running-behavysis_pipeline","title":"Running behavysis_pipeline","text":"<p>To install <code>behavysis_pipeline</code>, follow these instructions.</p> <p>To run <code>behavysis_pipeline</code>, follow these these instructions.</p>"}]}